{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "from string import punctuation\n",
    "from os.path import exists, getsize\n",
    "from os import listdir, remove\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "import tracemalloc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/monitoring-memory-usage-of-a-running-python-program/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/max/datasets/flashback-mat.xml\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "threshold = 200\n",
    "with open(file_path, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < threshold:\n",
    "            print(line.strip(\"\\n\"))\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "corpus\n",
    "    thread\n",
    "        text #date\n",
    "            paragraph\n",
    "                sentence\n",
    "                    token\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def file_runner(file_path):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    tracemalloc.start()\n",
    "    memory0 = (0, 0)\n",
    "    with open(file_path, \"r\") as xml:\n",
    "        \n",
    "        for i, line in enumerate(xml):\n",
    "\n",
    "            if i % 1000000 == 0:\n",
    "                t = time.time() - t0\n",
    "                m = int(t / 60)\n",
    "                s = t % 60\n",
    "                memory1 = tracemalloc.get_traced_memory()\n",
    "                memory  = round(memory1[0]/1000, 1)\n",
    "                memory_delta = round((memory1[0]-memory0[0])/1000, 1)\n",
    "                memory0 = memory1\n",
    "                print(f\"{i} lines processed so far. {m} minutes and {s:.1f} seconds have passed. Memory={memory} KB (+{memory_delta} KB)\", end = \"\\r\")    \n",
    "    print()\n",
    "    print(\"Lines, total:\", i)\n",
    "    print(\"Done!\")\n",
    "    tracemalloc.stop()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "file_runner(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 119 000 000 lines passed in 4 minutes and 19 seconds."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def year_finder(xml_file_path):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    tracemalloc.start()\n",
    "    memory0 = (0, 0)\n",
    "    \n",
    "    xml_parser = ET.XMLPullParser(['start'])\n",
    "    years = set()\n",
    "    \n",
    "    with open(file_path, \"r\") as xml:\n",
    "        \n",
    "        for i, line in enumerate(xml):\n",
    "\n",
    "            if i % 1000000 == 0:\n",
    "                del xml_parser\n",
    "                \n",
    "                t = time.time() - t0\n",
    "                m = int(t / 60)\n",
    "                s = t % 60\n",
    "                memory1 = tracemalloc.get_traced_memory()\n",
    "                memory  = round(memory1[0]/1000000, 1)\n",
    "                memory_delta = round((memory1[0]-memory0[0])/1000000, 1)\n",
    "                memory0 = memory1\n",
    "                \n",
    "                xml_parser = ET.XMLPullParser(['start'])\n",
    "                \n",
    "                print(f\"{i} lines processed so far. {m} minutes and {s:.1f} seconds have passed. Memory={memory} MB (+{memory_delta} MB)\", end = \"\\r\")    \n",
    "            \n",
    "            if re.search(\"<text.*?>\", line) != None:\n",
    "                \n",
    "                xml_parser.feed(line)\n",
    "                #xml_parser.read_events()\n",
    "                #date = [tag for _, tag in xml_parser.read_events()][0].attrib[\"date\"]\n",
    "                for _, tag in xml_parser.read_events():\n",
    "                    date = tag.attrib[\"date\"]\n",
    "                    tag.clear()\n",
    "                \n",
    "                year = date.split()[0].split(\"-\")[0] # \"2012-08-17 10:22\"\n",
    "                years.add(year)\n",
    "                \n",
    "                \n",
    "    \n",
    "    print()\n",
    "    print(\"Lines, total:\", i)\n",
    "    \n",
    "    tracemalloc.stop()\n",
    "    \n",
    "    return sorted(list(years))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "year_finder(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from 2000 to 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_path = Path(\"utils/stopwords-sv.txt\")\n",
    "with open(stopwords_path, mode = \"r\") as f:\n",
    "    stopwords = [stopword.strip(\"\\n\") for stopword in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inititialize(directory, start_year, stop_year, suffix):\n",
    "    \"\"\"\n",
    "    Creates a set of files to populate with examples.\n",
    "    WARNING: replaces existing files with empty ones.\n",
    "    \"\"\"\n",
    "    \n",
    "    directory = Path(directory)\n",
    "    \n",
    "    for year in range(start_year, stop_year + 1):\n",
    "        filename = f\"{year}.txt\" if suffix == \"\" else f\"{year}_suffix.txt\"\n",
    "        f = open(directory / filename, mode = \"w\")\n",
    "        f.close()        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sentences(batch, directory, suffix):\n",
    "    \"\"\"\n",
    "    Saves an example to the .txt file of its year.\n",
    "    Assumes initiation by `inititialize()`.\n",
    "    \"\"\"\n",
    "    \n",
    "    directory = Path(directory)\n",
    "    \n",
    "    for year, example_str in batch:\n",
    "        filename  = f\"{year}.txt\" if suffix == \"\" else f\"{year}_{suffix}.txt\"\n",
    "\n",
    "        with open(directory/filename, mode=\"a\") as f:\n",
    "            f.write(example_str + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(directory):\n",
    "    \"\"\"\n",
    "    Removes empty files.\n",
    "    \"\"\"\n",
    "    \n",
    "    for file in listdir(directory):\n",
    "        if getsize(directory / file) == 0:\n",
    "            remove(directory / file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example,\n",
    "               remove_stopwords = False,\n",
    "               stopwords = None,\n",
    "               remove_numbers = True,\n",
    "               remove_urls = True,\n",
    "               remove_punctuations = True,\n",
    "               min_tok_utterances = None,\n",
    "               lower = True):\n",
    "    \"\"\" \n",
    "    Preprocess a list of words and returns a string.\n",
    "    \"\"\"\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        assert isinstance(stopwords, list) or isinstance(stopwords, set), \"You have selected to use stopwords, but no stopword list is provided.\"\n",
    "\n",
    "        example = [word for word in example if word not in stopwords]\n",
    "        \n",
    "    if remove_punctuations:\n",
    "        example = [token for token in example if not token in punctuation]\n",
    "        \n",
    "    if min_tok_utterances != None:\n",
    "        if len(example) <= min_tok_utterances:\n",
    "            return []\n",
    "        \n",
    "    example = \" \".join(example)\n",
    "    \n",
    "    if lower:\n",
    "        example = example.lower()\n",
    "    \n",
    "    if remove_numbers:\n",
    "        example = re.sub(r\"[0-9]+\", \"\", example)\n",
    "    \n",
    "    if remove_urls:\n",
    "        example = re.sub(r\"https?://.*\", \"\", example)\n",
    "        example = re.sub(r\"www\\..*\", \"\", example)\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_temporal_corpus(xml_file_path, \n",
    "                          directory_out, \n",
    "                          stop = 400,\n",
    "                          interval = 100000,\n",
    "                          start_year = 2000, \n",
    "                          end_year = 2022,\n",
    "                          buffer_limit = 500, \n",
    "                          suffix = \"\"):\n",
    "    \"\"\"\n",
    "    Builds temporal corpus from xml-file; one file per year.\n",
    "    \"\"\"\n",
    "    \n",
    "    t0 = time.time()\n",
    "    tracemalloc.start()\n",
    "    memory0 = (0,0)\n",
    "\n",
    "    xml_parser = ET.XMLPullParser(['start'])    \n",
    "    \n",
    "    directory_out = Path(directory_out)\n",
    "\n",
    "    collector = []\n",
    "\n",
    "    counter = {str(year): {\"examples\": 0, \"word_tokens\": 0} for year in range(start_year, end_year + 1)}\n",
    "    \n",
    "    inititialize(directory_out, start_year, end_year, suffix)\n",
    "\n",
    "    with open(xml_file_path, \"r\") as xml:\n",
    "        \n",
    "        for i, line in enumerate(xml):\n",
    "            \n",
    "            if i % interval == 0:\n",
    "                \n",
    "                t = time.time() - t0\n",
    "                m = int(t / 60)\n",
    "                s = t % 60\n",
    "                \n",
    "                norm, unit = (1000000, \"MB\")\n",
    "                memory1 = tracemalloc.get_traced_memory()\n",
    "                memory  = round(memory1[0]/norm, 1)\n",
    "                memory_delta = round((memory1[0]-memory0[0])/norm, 1)\n",
    "                memory0 = memory1                \n",
    "                \n",
    "                print(f\"{i} lines; in {m} m, {s:.1f} s; memory={memory} {unit} (+{memory_delta} {unit}) \", end = \"\\r\")\n",
    "                \n",
    "                # this manouver is to avoid memory explosion\n",
    "                del xml_parser\n",
    "                xml_parser = ET.XMLPullParser(['start'])                \n",
    "\n",
    "            if stop != None:\n",
    "                if i > stop:\n",
    "                    break\n",
    "            \n",
    "            if re.search(\"<text.*?>\", line) != None:\n",
    "                xml_parser.feed(line)\n",
    "                date = [tag for _, tag in xml_parser.read_events()][0].attrib[\"date\"]\n",
    "                year = date.split()[0].split(\"-\")[0] # \"2012-08-17 10:22\"\n",
    "                continue\n",
    "            \n",
    "#             if restricted_to_year != None:\n",
    "#                 if year != restricted_to_year:\n",
    "#                     continue\n",
    "            \n",
    "            if re.search(\"<sentence.*?>\", line) != None:\n",
    "                raw = list()\n",
    "                #lem = list() # some inspection suggests that lemmas does not work that well; some very strange cases\n",
    "                #pos = list()\n",
    "                continue\n",
    "\n",
    "            if re.search(\"<token.*?>\", line) != None:\n",
    "                #print(line)\n",
    "                xml_parser.feed(\"<root>\"+line) # fake root to avoid ParseError\n",
    "                tag = [tag for _, tag in xml_parser.read_events()][-1]\n",
    "                #print(tag.text)\n",
    "                raw.append(tag.text)\n",
    "                #lem.append(re.sub(r\"\\|\", \"\", tag.attrib[\"lemma\"]))\n",
    "                #pos.append(tag.attrib[\"pos\"])\n",
    "                continue\n",
    "\n",
    "            if re.search(\"</sentence>\", line) != None:\n",
    "                #print(raw)\n",
    "                example = preprocess(raw)    # consider other parameters: pos, lemmas\n",
    "                if example == []:\n",
    "                    continue\n",
    "                counter[year][\"examples\"] += 1\n",
    "                counter[year][\"word_tokens\"] += len(example.split())\n",
    "                collector.append((year, example))\n",
    "\n",
    "            if len(collector) == buffer_limit:      \n",
    "                save_sentences(batch = collector, directory = directory_out, suffix = suffix)\n",
    "                collector.clear()\n",
    "            \n",
    "            # Without collector there will be alot of opening and closenings :|    \n",
    "            # Also, to use `with open()` for multiple files would require too many files open \n",
    "\n",
    "    if collector != []:\n",
    "        save_sentences(batch = collector, directory = directory_out, suffix = suffix)\n",
    "        \n",
    "    \n",
    "    counter = {year: counts for year, counts in counter.items() if sum(counts.values()) != 0}\n",
    "    with open(\"counter.log\", \"w\") as log:\n",
    "        log.write(json.dumps(counter))\n",
    "    \n",
    "    clean_up(directory_out)\n",
    "    \n",
    "    print()\n",
    "    print(\"Lines, total:\", i)\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    tracemalloc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119000000 lines; in 82 m, 13.5 s; memory=124.1 MB (+-0.6 MB) \n",
      "Lines, total: 119054753\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "build_temporal_corpus(xml_file_path = file_path, \n",
    "                      directory_out = \"diamat1\",\n",
    "                      stop=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -preprocessing -saving 6M lines: 01:51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old code for `build_temporal_corpus` \n",
    "Old code implements `xml.etree.ElementTree.iterparse()`, but this seems not to be a proper iterator. The computer gets parlyzed by memory overload."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def build_temporal_corpus(xml_file_path, \n",
    "                          directory_out, \n",
    "                          stop = 400, \n",
    "                          buffer_limit = 1000, \n",
    "                          start_year = 1990, \n",
    "                          end_year = 2023, \n",
    "                          suffix = \"\"):\n",
    "    \"\"\"\n",
    "    Builds temporal corpus from xml-file; one file per year.\n",
    "    \"\"\"\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    directory_out = Path(directory_out)\n",
    "\n",
    "    collector = []\n",
    "\n",
    "    inititialize(directory_out, start_year, end_year, suffix)\n",
    "    \n",
    "    counter = {str(year): {\"examples\": 0, \"word_tokens\": 0} for year in range(start_year, end_year + 1)}\n",
    "\n",
    "    #collect = False\n",
    "    for i, (event, tag) in enumerate(ET.iterparse(xml_file_path, events = [\"start\", \"end\"])):\n",
    "        \n",
    "        if i % 10000 == 0:\n",
    "            t = time.time() - t0\n",
    "            m = int(t / 60)\n",
    "            s = t % 60\n",
    "            print(f\"{i} events processed so far. {m} minutes and {s:.1f} seconds have passed.\", end = \"\\r\")\n",
    "        \n",
    "        \n",
    "        if stop != None:\n",
    "            if i > stop:\n",
    "                break\n",
    "\n",
    "        if event == \"start\" and tag.tag == \"text\":\n",
    "            date = tag.attrib[\"date\"]\n",
    "            year = date.split()[0].split(\"-\")[0] # \"2012-08-17 10:22\"\n",
    "            continue\n",
    "\n",
    "        if event == \"start\" and tag.tag == \"sentence\":\n",
    "            raw = list()\n",
    "            #lem = list() # some inspection suggests that lemmas does not work that well; some very strange cases\n",
    "            #pos = list()\n",
    "            continue\n",
    "\n",
    "        if event == \"end\" and tag.tag == \"token\":\n",
    "            raw.append(tag.text)\n",
    "            #lem.append(re.sub(r\"\\|\", \"\", tag.attrib[\"lemma\"]))\n",
    "            #pos.append(tag.attrib[\"pos\"])\n",
    "            continue\n",
    "\n",
    "        if event == \"end\" and tag.tag == \"sentence\":\n",
    "            example = preprocess(raw)    # consider other parameters: pos, lemmas\n",
    "            counter[year][\"examples\"] += 1\n",
    "            counter[year][\"word_tokens\"] += len(example.split())\n",
    "            collector.append((year, example))\n",
    "\n",
    "        if len(collector) == buffer_limit:      \n",
    "            save_sentences(batch = collector, directory = directory_out, suffix = suffix)\n",
    "            collector.clear()\n",
    "            \n",
    "        # Otherwise there will be alot of opening and closenings :|    \n",
    "        # To use with open for multiple files would require too many files open \n",
    "\n",
    "    if collector != []:\n",
    "        save_sentences(batch = collector, directory = directory_out, suffix = suffix)\n",
    "        \n",
    "    clean_up(directory_out)\n",
    "    counter = {year: counts for year, counts in counter.items() if sum(counts.values()) != 0}\n",
    "    with open(\"counter.log\", \"w\") as log:\n",
    "        log.write(json.dumps(counter))\n",
    "    \n",
    "    print()\n",
    "    print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
