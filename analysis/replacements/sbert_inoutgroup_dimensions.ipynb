{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Builds vectors of ***ingroup*** and ***outgroup***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** for Sentence-BERT approach, any *A-strategy* other than **really naive** does not make sense. E.g. consider exclusion due to overlap; exclusion of replacements \"kriminella invandrare\" because \"kriminella\" is found in replacements of both ingroup and outgroup is ***too strong***, *especially for the out-group in this case* (\"kriminella gäng\" is the most common ioutgroup replacment of *förortsgäng*). Perhaps, it is still worth testing the different forms of A-strategies as a form of \"noise reduction\". \n",
    "\n",
    "For the SGNS approach, A-strategies arguably are of importance, if we not going to build \"sentence vectors\" from concatenating word vectors. However, this might point to the limitations of SGNS for thi task, as it is not very good at representing phrases. \n",
    "\n",
    "Way forward: build in the possibility to use *A-selection* for SBERT, but focus on results for **really naive**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procedure (psuedo code)\n",
    "```\n",
    "DWEs = {förortsgäng, återvandring, globalist, berika}\n",
    "selection_strategies = {really_naive, naive_no_overlap, top1, top3, ...}\n",
    "models = {sbert_kb, ...}\n",
    "\n",
    "for DWE in DWEs:\n",
    "    for strategy in selection_strategies:\n",
    "        for model in models:\n",
    "\n",
    "            IN_vectors, OUT_vectors = select(replacement_vectors_of_dwe, strategy)\n",
    "            avg_IN_vec = mean(IN_vectors)\n",
    "            avg_OUT_vec = mean(OUT_vectors)\n",
    "\n",
    "            for year in years:\n",
    "                dwe_vect_at_year = get_vec(DWE)\n",
    "\n",
    "                IN_dimension_mean = cosine_similarity(avg_IN_vec, dwe_vect_at_year)\n",
    "                IN_dimension_pairwise_mean = mean(cosine_similarity(IN_vectors, dwe_vect_at_year))\n",
    "\n",
    "                OUT_dimension_mean = cosine_similarity(avg_OUT_vec, dwe_vect_at_year)\n",
    "                OUT_dimension_pairwise_mean = mean(cosine_similarity(OUT_vectors, dwe_vect_at_year))  \n",
    "\n",
    "                norm_dimension_mean = normalizer(IN_dimension_mean, OUT_dimension_mean)\n",
    "                norm_dimension_pariwise_mean = normalizer(IN_dimension_pairwise_mean, OUT_dimension_pairwise_mean)\n",
    "\n",
    "                # e.g. softmax or simply normalize(x, y) = x / (x+y)\n",
    "```\n",
    "\n",
    "#### Will get you something like (for each model)...\n",
    "\n",
    "|DWE            |Selection strategy|Method Dimension|Year<sub>1</sub>|...|Year<sub>*n*</sub>|\n",
    "|---------------|------------------|----------------|----------------|---|------------------|\n",
    "|DWE<sub>1</sub>|Really naive      |Mean            |...             |...|...               |\n",
    "|DWE<sub>1</sub>|Really naive      |Pairwise mean   |...             |...|...               |\n",
    "|DWE<sub>1</sub>|Really naive      |Normalized      |...             |...|...               |\n",
    "|DWE<sub>1</sub>|Naive no overlap  |Mean            |...             |...|...               |\n",
    "|DWE<sub>1</sub>|Naive no overlap  |Pairwise mean   |...             |...|...               |\n",
    "|DWE<sub>1</sub>|Naive no overlap  |Normalized      |...             |...|...               |\n",
    "|DWE<sub>1</sub>|Top1              |...             |...             |...|...               |\n",
    "|...            |...               |...             |...             |...|...               |\n",
    "|DWE<sub>2</sub>|...               |...             |...             |...|...               |\n",
    "\n",
    "#### Aditional considerations\n",
    "* Finetuning\n",
    "* Correlation with semantic change rates\n",
    "    * Spearman, Pearson\n",
    "    * Naive, Rectified\n",
    "* Correlation with (normalized) frequency\n",
    "    * Spearman, Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "from pathlib import Path\n",
    "from sklearn.utils.extmath import softmax\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyness(trg, ref, min_frq = 3, verbose = True): # Consider metric\n",
    "    \n",
    "    d = dict()\n",
    "    \n",
    "#     trg_tot = sum(trg.values())\n",
    "#     ref_tot = sum(ref.values())\n",
    "    trg_tot = len(trg)\n",
    "    ref_tot = len(ref)\n",
    "    \n",
    "    for w in trg.keys():\n",
    "        if trg[w] < min_frq:\n",
    "            continue\n",
    "        if w in ref:\n",
    "            d[w] = (trg[w] / trg_tot) / (ref[w] / ref_tot) # Odds Ratio (OR)\n",
    "        else:\n",
    "            d[w] = np.inf\n",
    "    \n",
    "    if verbose:\n",
    "        for word, trg_freq, keyness  in sorted([(w, trg[w], k) for w, k in d.items()], key = lambda x: x[1], reverse = True)[:20]:\n",
    "            if word in ref:\n",
    "                ref_freq = ref[word]\n",
    "            else:\n",
    "                ref_freq = 0\n",
    "            print(f\"{word:<20}{trg_freq:<4}{(trg_freq/trg_tot):<6.3f}{ref_freq:<4}{(ref_freq/ref_tot):<6.3f}{keyness:.4}\")        \n",
    "    \n",
    "    return d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect(\n",
    "    df,            # Replacement Dataframe\n",
    "    dwe,           # Dog Whistle Expression\n",
    "    meaning,       # 1 for ingroup, 2 for outgroup\n",
    "    phase,         # 1 for first phase of data collection, 2 for second phase\n",
    "    sw = None,     # stopwords\n",
    "    punct = None,  # remove punctuations\n",
    "    verbose = True,\n",
    "    multi = False, # Keep the multi-word units of the replacements\n",
    "    rel_freq = False # use relative frequncies freq / no. of documents\n",
    "):\n",
    "    \n",
    "    counter = Counter()\n",
    "    \n",
    "    if type(df) == pd.DataFrame:\n",
    "        column = df.loc[df[f\"{dwe}_w{phase}_C\"] == meaning, f\"{dwe}_text_w{phase}\"]\n",
    "    else:\n",
    "        column = df\n",
    "    \n",
    "    for x in column:\n",
    "        if punct != None:\n",
    "            for p in punct:\n",
    "                x = x.replace(p, \"\")\n",
    "        x = x.split()\n",
    "        if sw != None:\n",
    "            x = [w for w in x if w not in sw]\n",
    "        \n",
    "        if multi:\n",
    "            x = [\"_\".join(x)]\n",
    "        \n",
    "        counter.update(set(x)) # Obs. terms are only counted once per \"document\"\n",
    "    \n",
    "    if rel_freq:\n",
    "        counter = Counter({w: c/len(column) for w,c in counter.items()})\n",
    "        \n",
    "    if verbose:\n",
    "        for w, f in sorted(counter.items(), key = lambda x: x[1], reverse = True)[:15]:\n",
    "            print(f\"{w:<30}{f}\")\n",
    "        print(\"-----------------------\")\n",
    "        print(\"Total no. of types:\", len(counter))\n",
    "    \n",
    "    \n",
    "\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_A(\n",
    "    df,             # Replacement Dataframe\n",
    "    dwe,            # Dog Whistle Expression\n",
    "    phase = \"both\", # 1 for first phase of data collection, 2 for second phase, \"both\" for both\n",
    "    sw = None,      # stopwords\n",
    "    punct = None,   # remove punctuations\n",
    "    k = None,\n",
    "    min_freq = None,\n",
    "    min_OR = None,\n",
    "    empty_intersect = False\n",
    "):\n",
    "    \n",
    "    if type(k) == tuple:\n",
    "        k_in, k_out = k\n",
    "    else:\n",
    "        k_in  = k\n",
    "        k_out = k\n",
    "    if type(min_freq) == tuple:\n",
    "        min_freq_in, min_freq_out = min_freq\n",
    "    else:\n",
    "        min_freq_in  = min_freq\n",
    "        min_freq_out = min_freq\n",
    "    if type(min_OR) == tuple:\n",
    "        min_OR_in, min_OR_out = min_OR\n",
    "    else:\n",
    "        min_OR_in  = min_OR\n",
    "        min_OR_out = min_OR\n",
    "    \n",
    "    if phase == \"both\":\n",
    "        x = pd.concat([\n",
    "            df.loc[df[f\"{dwe}_w{1}_C\"] == 1, f\"{dwe}_text_w{1}\"],\n",
    "            df.loc[df[f\"{dwe}_w{2}_C\"] == 1, f\"{dwe}_text_w{2}\"]\n",
    "        ]).to_list()\n",
    "                \n",
    "        y = pd.concat([\n",
    "            df.loc[df[f\"{dwe}_w{1}_C\"] == 2, f\"{dwe}_text_w{1}\"],\n",
    "            df.loc[df[f\"{dwe}_w{2}_C\"] == 2, f\"{dwe}_text_w{2}\"]\n",
    "        ]).to_list()\n",
    "\n",
    "        ingroup = inspect(x, dwe, None, None, sw, punct, verbose = False, rel_freq = True)\n",
    "        outgroup = inspect(y, dwe, None, None, sw, punct, verbose = False, rel_freq = True)\n",
    "#         _ = inspect(x, dwe, None, None, sw, punct, multi = True)   \n",
    "#         _ = inspect(y, dwe, None, None, sw, punct, multi = True)   \n",
    "\n",
    "        keyness_in2out = keyness(ingroup, outgroup, verbose = False, min_frq = -1)\n",
    "        keyness_out2in = keyness(outgroup, ingroup, verbose = False, min_frq = -1)\n",
    "        \n",
    "    else:    \n",
    "    \n",
    "        ingroup = inspect(df, dwe, 1, phase, sw, punct, verbose = False, rel_freq = True)\n",
    "        outgroup = inspect(df, dwe, 2, phase, sw, punct, verbose = False, rel_freq = True)\n",
    "#         _ = inspect(df, dwe, 1, phase, sw, punct, multi = True)\n",
    "#         _ = inspect(df, dwe, 2, phase, sw, punct, multi = True)\n",
    "        keyness_in2out = keyness(ingroup, outgroup, verbose = False, min_frq = -1)\n",
    "        keyness_out2in = keyness(outgroup, ingroup, verbose = False, min_frq = -1)\n",
    "    \n",
    "    A_in  = [w for w in ingroup.keys()]\n",
    "    A_out = [w for w in outgroup.keys()]\n",
    "    #print(A_out)\n",
    "    \n",
    "    if empty_intersect:\n",
    "        A_in  = [w for w in A_in if w not in outgroup.keys()]\n",
    "        A_out = [w for w in A_out if w not in ingroup.keys()]\n",
    "        \n",
    "    if min_freq != None:\n",
    "        A_in  = [w for w in A_in if ingroup[w] >= min_freq_in]\n",
    "        A_out = [w for w in A_out if outgroup[w] >= min_freq_out]\n",
    "    \n",
    "    #print(A_out)\n",
    "    \n",
    "    if min_OR != None:\n",
    "        A_in  = [w for w in A_in if keyness_in2out[w] >= min_OR_in]\n",
    "        A_out = [w for w in A_out if keyness_out2in[w] >= min_OR_out] # too strict to have the same threshold for both\n",
    "        \n",
    "    #print(A_out)    \n",
    "    \n",
    "    if k != None:\n",
    "        A_in  = [w for w,_ in sorted(ingroup.items(), key = lambda x: x[1], reverse = True) if w in A_in][:k_in]\n",
    "        A_out = [w for w,_ in sorted(outgroup.items(), key = lambda x: x[1], reverse = True) if w in A_out][:k_out]\n",
    "    \n",
    "    \n",
    "    return A_in, A_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matcher(string, A_list, punct = [\",\", \"?\", \".\", \"!\", \";\"]): \n",
    "    # add more sophistication?'\n",
    "    # punct should match those of Select_A()\n",
    "    \n",
    "    match = False\n",
    "\n",
    "    for p in punct:\n",
    "        string = string.replace(p, \"\")\n",
    "    for w in string.split(\" \"):\n",
    "        if w in A_list:\n",
    "            match = True\n",
    "            \n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_replacements(dwe, meaning, rnd, model, data_path):\n",
    "    \n",
    "    with open(data_path / dwe / meaning / rnd / \"replacements.txt\") as f:\n",
    "        idx_t, text = zip(*[tuple(line.strip(\"\\n\").split(\"\\t\")) for line in f.readlines()[1:]]) # Obs! skip first line\n",
    "    with open(data_path / dwe / meaning / rnd / \"vectors\" / model / \"vecs.txt\") as f:\n",
    "        lines = [tuple(line.strip(\"\\n\").split(\"\\t\")) for line in f.readlines()]\n",
    "        lines = [(idx, [float(v) for v in vec.split()]) for idx, vec in lines]\n",
    "        #print(lines[:2])\n",
    "        idx_v, vectors = zip(*lines)\n",
    "    \n",
    "    assert idx_t == idx_v, \"Vectors and text are not aligned.\"\n",
    "    \n",
    "#     for t, v in zip(idx_t, idx_v):\n",
    "#         if t != v:\n",
    "#             print(t,v)\n",
    "\n",
    "    #print(\"MUUUU\", vectors[:2])\n",
    "    \n",
    "    return [(text, vec) for text, vec in zip(text, vectors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_vec(data_path, dwe, Aigt, Aogt, model, rounds = [\"first_round\", \"second_round\"]):\n",
    "    \"\"\"\n",
    "    Based on A for the ingroup and the outgroup, collects vectors of the replacements that map to A. \n",
    "    Mapping between A and vectors of replacements uses `matcher()`.\n",
    "    \"\"\"\n",
    "    \n",
    "    igt_vectors = []\n",
    "    ogt_vectors = []\n",
    "    \n",
    "    for meaning in [\"ingroup\", \"outgroup\"]:\n",
    "        for rnd in rounds: # [\"first_round\", \"second_round\"] or just one of them\n",
    "            for replacement, vector in load_replacements(dwe, meaning, rnd, model, data_path):\n",
    "                if meaning == \"ingroup\":\n",
    "                    if matcher(replacement, Aigt): # punctuation\n",
    "                        igt_vectors.append(vector)\n",
    "                else:\n",
    "                    if matcher(replacement, Aogt): # punctuation\n",
    "                        ogt_vectors.append(vector)\n",
    "    \n",
    "    return np.array(igt_vectors), np.array(ogt_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     df,            # Replacement Dataframe\n",
    "#     dwe,           # Dog Whistle Expression\n",
    "#     phase,         # 1 for first phase of data collection, 2 for second phase, \"both\" for both\n",
    "#     sw = None,     # stopwords\n",
    "#     punct = None,  # remove punctuations\n",
    "#     k = None,\n",
    "#     min_freq = None,\n",
    "#     min_OR = None,\n",
    "#     empty_intersect = False\n",
    "\n",
    "def select2vec(mode, dwe, wh_rnds, model, path_dfA, stopwords, punct, data_path, verbose = True):\n",
    "    \"\"\"\n",
    "    Based on a strategy, i.e. `mode`, Select A and returns vectors of replacments that map to A. \n",
    "    Uses `select_A()` and `collect_vec()`.\n",
    "    \"\"\"\n",
    "\n",
    "    if mode == \"rn\":    # Really naive; probably the most sensible for SBERT\n",
    "        \n",
    "        igt_vectors = []\n",
    "        for rnd in wh_rnds:\n",
    "            _, vecs = zip(*load_replacements(dwe, \"ingroup\", rnd, model, data_path))\n",
    "            \n",
    "            #print(vecs[1]. len(vecs[1]))\n",
    "            \n",
    "            igt_vectors.extend(vecs)\n",
    "        \n",
    "        ogt_vectors = []\n",
    "        for rnd in wh_rnds:\n",
    "            _, vecs = zip(*load_replacements(dwe, \"outgroup\", rnd, model, data_path))\n",
    "            ogt_vectors.extend(vecs)  \n",
    "            \n",
    "#         print(ogt_vectors[:2])\n",
    "\n",
    "        return np.array(igt_vectors), np.array(ogt_vectors)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        dfA = pd.read_csv(path_dfA, sep=\"\\t\") # check parameters\n",
    "        dfA = dfA.applymap(lambda s: s.lower() if type(s) == str else s)\n",
    "        \n",
    "        if wh_rnds == [\"first_round\"]:\n",
    "            PHASE = 1\n",
    "        if wh_rnds == [\"second_round\"]:\n",
    "            PHASE = 2\n",
    "        else:\n",
    "            PHASE = \"both\"\n",
    "        \n",
    "        if mode == \"nno\":   # Naive No Overlap\n",
    "            Aigt, Aogt = select_A(\n",
    "                df = dfA, \n",
    "                dwe = dwe, \n",
    "                phase = PHASE, \n",
    "                sw = stopwords, \n",
    "                punct = punct, \n",
    "                empty_intersect = True)\n",
    "\n",
    "        if mode == \"top1\":  # Top 1 (no overlap)\n",
    "            Aigt, Aogt = select_A(\n",
    "                df = dfA, \n",
    "                dwe = dwe,\n",
    "                phase = PHASE,\n",
    "                sw = stopwords,\n",
    "                punct = punct,\n",
    "                k = 1,\n",
    "                empty_intersect = True\n",
    "            )\n",
    "\n",
    "        if mode == \"top3\":  # Top 3 (no overlap)\n",
    "            Aigt, Aogt = select_A(\n",
    "                df = dfA, \n",
    "                dwe = dwe,\n",
    "                phase = PHASE,\n",
    "                sw = stopwords,\n",
    "                punct = punct,\n",
    "                k = 3,\n",
    "                empty_intersect = True\n",
    "            )\n",
    "\n",
    "        if mode == \"ms1\":    # Multiple Selection; threshold ... \n",
    "            Aigt, Aogt = select_A(\n",
    "                df = dfA, \n",
    "                dwe = dwe,\n",
    "                phase = PHASE,\n",
    "                sw = stopwords,\n",
    "                punct = punct,\n",
    "                k = 3,\n",
    "                min_OR = 2.0,\n",
    "                empty_intersect = False\n",
    "            )\n",
    "        \n",
    "        if verbose:\n",
    "            if len(Aigt) < 4:\n",
    "                print(\"\\t\", mode)\n",
    "                print(\"\\tAigt:\", \", \".join(Aigt))\n",
    "                print(\"\\tAogt:\", \", \".join(Aogt))\n",
    "        \n",
    "        \n",
    "        if wh_rnds == 1:\n",
    "            rounds = [\"first_round\"]\n",
    "        elif wh_rnds == 2:\n",
    "            rounds = [\"second_round\"]\n",
    "        else: # i.e. wh_rnds == \"both\"\n",
    "            rounds = [\"first_round\", \"second_round\"]\n",
    "\n",
    "        return collect_vec(data_path, dwe, Aigt, Aogt, model, rounds)\n",
    "        #dwe, Aigt, Aogt, model, rounds = [\"first_round\", \"second_round\"], data_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PairwiseMeanSimilarity(v, v_list):\n",
    "    \n",
    "    pairwise = cosine_similarity(v, v_list)\n",
    "    pairwise_mean = pairwise.mean()\n",
    "    \n",
    "    return pairwise_mean\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_corpus_vector(dwe, year, model, path):\n",
    "    \n",
    "    with open(path / model / \"centroid\" / f\"{year}.txt\") as f:\n",
    "        lines = [line.strip(\"\\n\") for line in f.readlines()]\n",
    "    \n",
    "    for line in lines:\n",
    "        term, vector = tuple(line.split(\"\\t\"))\n",
    "        if term == dwe:\n",
    "            vector = [float(v) for v in vector.split()]\n",
    "            return np.array(vector)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_string(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repl_dwe(dwe, rule = None, verbose = True):\n",
    "    \n",
    "    if rule != None:\n",
    "        return rule[dwe]\n",
    "    else: # infer!\n",
    "        potential_dwes = [\"forortsgang\", \"aterinvandring\", \"berikar\", \"globalister\"]\n",
    "        \n",
    "        dwe = dwe.split(\"_\")[-1]\n",
    "        \n",
    "        best_score = 0\n",
    "        best_guess = None\n",
    "        \n",
    "        for candidate in potential_dwes:\n",
    "            score = similar_string(dwe, candidate)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_guess = candidate\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\tInference for {dwe}: {best_guess} (score = {best_score:.2f}).\")\n",
    "        \n",
    "        return best_guess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self\n",
    "        \n",
    "#         self.dwes = dwes\n",
    "#         self.wh_rounds = rounds\n",
    "#         self.dfA_path = dfA_path\n",
    "#         self.strategies = strategies\n",
    "#         self.years = years\n",
    "#         self_measures = \n",
    "#         self.add_correlations\n",
    "#         self.model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    long = len(dwes) * len(strategies) * len(measures) X \n",
    "           |{dwe, strategy, measure, year1, ... year2}|\n",
    "           # if add_correlations:\n",
    "           #    |{dwe, strategy, measure, year1, ... year2, r_naive, r_rect, rho_naive, rho_rect, r_fpm, rho_fpm}|\n",
    "    \n",
    "    wide = len(dwes) * len(strategies) X \n",
    "           |{dwe, strategy}| + len(measure) * len(years)\n",
    "           # no correlations\n",
    "           \n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sbert_builder(config):\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    methods = [\"I-cnt\", \"O-cnt\", \"cnt-ssc\", \"cnt-smx\", \"I-pwn\", \"O-pwn\", \"pwn-ssc\", \"pwn-smx\"]\n",
    "    # alternative: make this a config attribute\n",
    "    \n",
    "    years = [str(year) for year in range(config.first_year, config.last_year)]\n",
    "    years.sort()\n",
    "    \n",
    "    with open(config.stopwords) as f:\n",
    "        stopwords = [w.strip(\"\\n\") for w in f.readlines()]\n",
    "    \n",
    "    for progress, dwe in enumerate(config.dwes, start = 1):\n",
    "        print(f\"Processing {progress} of {len(config.dwes)}: {dwe}.\")\n",
    "        dwe_in_replacement_test = repl_dwe(dwe)\n",
    "        for strategy in config.strategies:\n",
    "            \n",
    "            INGROUPvec, OUTGROUPvec = select2vec(\n",
    "                mode      = strategy, \n",
    "                dwe       = dwe_in_replacement_test, \n",
    "                wh_rnds   = config.wh_rounds, \n",
    "                model     = config.model, \n",
    "                path_dfA  = config.dfA_path, \n",
    "                stopwords = stopwords, \n",
    "                punct     = config.punct, \n",
    "                data_path = config.data_path\n",
    "            )\n",
    "            \n",
    "            #return INGROUPvec\n",
    "            \n",
    "            ING_centroid  = INGROUPvec.mean(axis=0)\n",
    "            OUTG_centroid = OUTGROUPvec.mean(axis=0)\n",
    "\n",
    "            d = {method: [] for method in methods}\n",
    "\n",
    "            for year in years:\n",
    "\n",
    "                vector = find_corpus_vector(dwe, year, config.model, config.path_corpus_vectors)\n",
    "                \n",
    "                #print(type(vector))\n",
    "\n",
    "                if type(vector) != np.ndarray:\n",
    "\n",
    "                    d[\"I-cnt\"].append(None)\n",
    "                    d[\"O-cnt\"].append(None)\n",
    "                    d[\"cnt-ssc\"].append(None)\n",
    "                    d[\"cnt-smx\"].append(None)\n",
    "                    d[\"I-pwn\"].append(None)\n",
    "                    d[\"O-pwn\"].append(None)\n",
    "                    d[\"pwn-ssc\"].append(None)\n",
    "                    d[\"pwn-smx\"].append(None)                    \n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    #print(vector.shape, ING_centroid.shape, OUTG_centroid.shape)\n",
    "                    \n",
    "#                     i_cnt = cosine_similarity(vector, ING_centroid)[0] \n",
    "#                     o_cnt = cosine_similarity(vector, OUTG_centroid)[0]\n",
    "                    \n",
    "                    i_cnt = cosine_similarity(vector.reshape(1,-1), ING_centroid.reshape(1,-1))[0][0] \n",
    "                    o_cnt = cosine_similarity(vector.reshape(1,-1), OUTG_centroid.reshape(1,-1))[0][0]\n",
    "\n",
    "                    i_pwn = PairwiseMeanSimilarity(vector.reshape(1, -1), INGROUPvec)\n",
    "                    o_pwn = PairwiseMeanSimilarity(vector.reshape(1, -1), OUTGROUPvec)\n",
    "                    \n",
    "                    #print(i_cnt, o_cnt, i_pwn, o_pwn)\n",
    "\n",
    "                    d[\"I-cnt\"].append(i_cnt)\n",
    "                    d[\"O-cnt\"].append(o_cnt) \n",
    "                    d[\"cnt-ssc\"].append(i_cnt / (i_cnt + o_cnt))\n",
    "                    d[\"cnt-smx\"].append(softmax([[i_cnt, o_cnt]])[0][0])\n",
    "\n",
    "                    d[\"I-pwn\"].append(i_pwn)\n",
    "                    d[\"O-pwn\"].append(o_pwn) \n",
    "                    d[\"pwn-ssc\"].append(i_pwn / (i_pwn + o_pwn))\n",
    "                    d[\"pwn-smx\"].append(softmax([[i_pwn, o_pwn]])[0][0])\n",
    "\n",
    "            if config.results_format == \"long\":\n",
    "                \n",
    "                for method in d.keys():\n",
    "                    line = [dwe, strategy, method]\n",
    "                    line.extend(d[method])\n",
    "#                     if config.add_correlations:\n",
    "#                         r_naive\n",
    "#                         r_rect\n",
    "#                         rho_naive\n",
    "#                         rho_rect\n",
    "#                         r_fpm\n",
    "#                         rho_fpm\n",
    "#                         N                    \n",
    "                    results.append(line)\n",
    "\n",
    "            else: # if results_format == \"wide\"\n",
    "                line = [dwe, strategy]\n",
    "                for method in d.keys():\n",
    "                    line.extend(d[method])\n",
    "                results.append(line)\n",
    "    \n",
    "    if config.results_format == \"long\":\n",
    "        features = [\"DWE\", \"A-Strategy\", \"Method\"] + years\n",
    "        if config.add_correlations:\n",
    "            additional_headings = [\"r_naive\", \"r_rect\", ...]\n",
    "            features.extend(additional_headings)\n",
    "        \n",
    "    \n",
    "    else: # if wide\n",
    "        features = [\"DWE\", \"A-Strategy\"]\n",
    "        for method in methods:\n",
    "            m = [f\"{method}_{year}\" for year in years]\n",
    "            features.extend(m)\n",
    "    \n",
    "    df = pd.DataFrame(results, columns = features)\n",
    "    \n",
    "    df.to_csv(config.results_path)\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    \n",
    "                    \n",
    "                \n",
    "####################################################################################                \n",
    "                \n",
    "#                 df.at[i, f\"I-cnt_{year}\"] = i_cnt\n",
    "#                 df.at[i, f\"O-cnt_{year}\"] = o_cnt \n",
    "#                 df.at[i, f\"cnt-ssc_{year}\"] = i_cnt / (i_cnt + o_cnt)\n",
    "#                 df.at[i, f\"cnt-smx_{year}\"] = softmax([i_cnt, o_cnt])[0]\n",
    "\n",
    "#                 df.at[i, f\"I-pwn_{year}\"] = i_pwn\n",
    "#                 df.at[i, f\"O-pwn_{year}\"] = o_pwn \n",
    "#                 df.at[i, f\"pwn-ssc_{year}\"] = i_pwn / (i_pwn + o_pwn)\n",
    "#                 df.at[i, f\"pwn-smx_{year}\"] = softmax([i_pwn, o_pwn])[0]\n",
    "            \n",
    "#         i += 1 # updates with every iteration of strategy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.first_year = 2000\n",
    "config.last_year  = 2022\n",
    "config.dwes       = [\n",
    "                    \"V1_berika\",\n",
    "                    \"N1_berikare\",\n",
    "                    \"V1_kulturberika\",\n",
    "                    \"N1_kulturberikare\",\n",
    "                    \"N1_globalist\",\n",
    "                    \"A1_globalistisk\",\n",
    "                    \"N1_återvandring\",\n",
    "                    \"V1_återvandra\",\n",
    "                    #\"V1_hjälpa_på_plats\",\n",
    "                    \"N1_förortsgäng\"\n",
    "                    ]\n",
    "config.strategies = [\"rn\", \"nno\", \"top1\", \"top3\", \"ms1\"]\n",
    "config.wh_rounds  = [\"first_round\", \"second_round\"]\n",
    "config.model      = \"sentence-bert-swedish-cased\"\n",
    "config.dfA_path   = Path(\"/home/max/Documents/research/replacement_data/panel_wide_onlyreplace.csv\")\n",
    "config.stopwords  = Path(\"../../data/utils/stopwords-sv.txt\")\n",
    "config.punct      = [\",\", \"?\", \".\", \"!\", \";\"]\n",
    "config.data_path   = Path(\"/home/max/Results/replacements/data\")\n",
    "config.path_corpus_vectors = Path(\"/home/max/Results/fb_pol-yearly-bert/\")\n",
    "config.results_format = \"long\"\n",
    "config.add_correlations = False\n",
    "config.results_path = Path(\"/home/max/Desktop/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 of 9: V1_berika.\n",
      "\tInference for berika: berikar (score = 0.92).\n",
      "\t top1\n",
      "\tAigt: förstör\n",
      "\tAogt: positivt\n",
      "\t top3\n",
      "\tAigt: förstör, utnyttjar, försämrar\n",
      "\tAogt: positivt, ger, gynnar\n",
      "\t ms1\n",
      "\tAigt: förstör, utnyttjar, försämrar\n",
      "\tAogt: tillför, bidrar, förbättrar\n",
      "Processing 2 of 9: N1_berikare.\n",
      "\tInference for berikare: berikar (score = 0.93).\n",
      "\t top1\n",
      "\tAigt: förstör\n",
      "\tAogt: positivt\n",
      "\t top3\n",
      "\tAigt: förstör, utnyttjar, försämrar\n",
      "\tAogt: positivt, ger, gynnar\n",
      "\t ms1\n",
      "\tAigt: förstör, utnyttjar, försämrar\n",
      "\tAogt: tillför, bidrar, förbättrar\n",
      "Processing 3 of 9: V1_kulturberika.\n",
      "\tInference for kulturberika: berikar (score = 0.63).\n",
      "\t top1\n",
      "\tAigt: förstör\n",
      "\tAogt: positivt\n",
      "\t top3\n",
      "\tAigt: förstör, utnyttjar, försämrar\n",
      "\tAogt: positivt, ger, gynnar\n",
      "\t ms1\n",
      "\tAigt: förstör, utnyttjar, försämrar\n",
      "\tAogt: tillför, bidrar, förbättrar\n",
      "Processing 4 of 9: N1_kulturberikare.\n",
      "\tInference for kulturberikare: berikar (score = 0.67).\n",
      "\t top1\n",
      "\tAigt: förstör\n",
      "\tAogt: positivt\n",
      "\t top3\n",
      "\tAigt: förstör, utnyttjar, försämrar\n",
      "\tAogt: positivt, ger, gynnar\n",
      "\t ms1\n",
      "\tAigt: förstör, utnyttjar, försämrar\n",
      "\tAogt: tillför, bidrar, förbättrar\n",
      "Processing 5 of 9: N1_globalist.\n",
      "\tInference for globalist: globalister (score = 0.90).\n",
      "\t top1\n",
      "\tAigt: eliten\n",
      "\tAogt: internationalister\n",
      "\t top3\n",
      "\tAigt: eliten, soros, elitister\n",
      "\tAogt: internationalister, världsmedborgare, internationellt\n",
      "\t ms1\n",
      "\tAigt: judar, elit, eliten\n",
      "\tAogt: internationalister, världsmedborgare, internationellt\n",
      "Processing 6 of 9: A1_globalistisk.\n",
      "\tInference for globalistisk: globalister (score = 0.78).\n",
      "\t top1\n",
      "\tAigt: eliten\n",
      "\tAogt: internationalister\n",
      "\t top3\n",
      "\tAigt: eliten, soros, elitister\n",
      "\tAogt: internationalister, världsmedborgare, internationellt\n",
      "\t ms1\n",
      "\tAigt: judar, elit, eliten\n",
      "\tAogt: internationalister, världsmedborgare, internationellt\n",
      "Processing 7 of 9: N1_återvandring.\n",
      "\tInference for återvandring: aterinvandring (score = 0.85).\n",
      "\t top1\n",
      "\tAigt: deportering\n",
      "\tAogt: möjlighet\n",
      "\t top3\n",
      "\tAigt: deportering, utvisningar, kasta\n",
      "\tAogt: möjlighet, hjälpa, hemvändning\n",
      "\t ms1\n",
      "\tAigt: utvisning, skicka, deportering\n",
      "\tAogt: flytta, återvända, hemland\n",
      "Processing 8 of 9: V1_återvandra.\n",
      "\tInference for återvandra: aterinvandring (score = 0.67).\n",
      "\t top1\n",
      "\tAigt: deportering\n",
      "\tAogt: möjlighet\n",
      "\t top3\n",
      "\tAigt: deportering, utvisningar, kasta\n",
      "\tAogt: möjlighet, hjälpa, hemvändning\n",
      "\t ms1\n",
      "\tAigt: utvisning, skicka, deportering\n",
      "\tAogt: flytta, återvända, hemland\n",
      "Processing 9 of 9: N1_förortsgäng.\n",
      "\tInference for förortsgäng: forortsgang (score = 0.82).\n",
      "\t top1\n",
      "\tAigt: invandrar\n",
      "\tAogt: ytterområden\n",
      "\t top3\n",
      "\tAigt: invandrar, invandrartäta, invandrade\n",
      "\tAogt: ytterområden, socioekonomiskt, vissa\n",
      "\t ms1\n",
      "\tAigt: invandrargäng, invandrare, invandrarungdomar\n",
      "\tAogt: utsatta, förorter, ungdomsgäng\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "sbert_builder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#X = sbert_builder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4650570548417855"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([[0.13,0.27]])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(\"ABC\")\n",
    "x.extend(list(\"DEF\"))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[1,2,3], [3,4,5]]).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for t in [\"mamma\", \"mu\"]:\n",
    "    df[\"name\"] = t\n",
    "    for m in [\"A\", \"B\", \"C\"]:\n",
    "        \n",
    "        for i in [1,2,3]:\n",
    "            for j in [10,20,30]:\n",
    "                df.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.at[0, \"B\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(None, index=np.arange(5), columns=[\"name\", \"A\", \"B\", \"C\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(mode, config):\n",
    "    \n",
    "    rpl_data = pd.read_csv()\n",
    "    \n",
    "    dwes = ...\n",
    "    \n",
    "    for strategy in A_list:\n",
    "        \n",
    "        A_ingroup, A_outgroup = select_A(\n",
    "            df,            # Replacement Dataframe\n",
    "            dwe,           # Dog Whistle Expression\n",
    "            phase,         # 1 for first phase of data collection, 2 for second phase, \"both\" for both\n",
    "            sw = None,     # stopwords\n",
    "            punct = None,  # remove punctuations\n",
    "            k = None,\n",
    "            min_freq = None,\n",
    "            min_OR = None,\n",
    "            empty_intersect = False\n",
    "        )\n",
    "    \n",
    "    \n",
    "    if mode == \"sgns\":\n",
    "        \n",
    "        \n",
    "    if mode == \"sbert\":\n",
    "        \n",
    "        sbert = config\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
