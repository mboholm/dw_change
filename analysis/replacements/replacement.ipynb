{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SGNS\n",
    "\n",
    "### How to build the ingroup/outgroup dimensions?\n",
    "Build one vector for each dimension based on a set of words *L* from the replacement experiments coded as \"ingroup\" or \"outgroup\".\n",
    "\n",
    "* Top word (|*L*| = 1), given some measure, e.g. frequency or \"keyness\" (see below)\n",
    "* Top *k* words (|*L*| = *k*), given some measure, e.g. frequency or \"keyness\"\n",
    "* Manual selection of *L*; can be more or less motivated by some measure, e.g. frequency or \"keyness\"\n",
    "\n",
    "Given *L* the association with a dog whistle expression (DWE) over time can be measured by:\n",
    "\n",
    "* **CENTROID**: The similarity of the vector of the DWE at *t* and the mean of vectors of words in *L* (at *t*)\n",
    "* **PAIRWISE**: The mean of similarity of the vector of the DWE (at *t*) and the vector of each word in *L* (given this latter approach, it is possible to use a *T*-test to compare if there is a difference in in- vs out-group association; the methodological implications of this shold be though through, though). **Does this matter?**\n",
    "\n",
    "\n",
    "### Morphology\n",
    "The replacement tests are done with specific forms, e.g. plural of *globalist*: \"globalister\". Hence, the responses (replacements) will typically align with the target form: \"elitist*er*\". Also *förortsgäng* is a collective noun with its replacement typically being plural (\"kriminella ungdom*ar*\"). These considerations poses a question about how (if at all) to expand *L* by including morphologically related forms when building the in/outgroup dimensions. \n",
    "\n",
    "### Should multiple words be combined (e.g., *skicka tillbaka*)? If so, how? \n",
    "(https://www.baeldung.com/cs/sentence-vectors-word2vec)\n",
    "\n",
    "For the SGNS models, it is easier if we ignore phrases and exclude \"stopwords\". But otherwise, the following methods can be considered to combine vectors:\n",
    "\n",
    "* addition\n",
    "* averaging\n",
    "* weigthed averaging\n",
    "\n",
    "### How to avoid overlap of dimensions? Keyness?\n",
    "For example, for *förortsgäng*, *kriminell* is a frequent term in replacements of both the ingroup and the outgroup. To handle this, some kind of keyword methodology can be considered (cf. corpus linguistics and meausures of \"keyness\", that considers the probability of term *t* in an ingoup vs. en outgroup \"corpus\").\n",
    "\n",
    "### Some definitions\n",
    "* *I*<sub>1</sub> =<sub>df.</sub> words in first replacement coded as ingroup meanings\n",
    "* *O*<sub>1</sub> =<sub>df.</sub> words in first replacement coded as outgroup meanings\n",
    "* *I*<sub>2</sub> =<sub>df.</sub> words in second replacement coded as ingroup meanings\n",
    "* *O*<sub>2</sub> =<sub>df.</sub> words in second replacement coded as outgroup meanings\n",
    "\n",
    "* *I*<sub>*both*</sub> =<sub>df.</sub> *I*<sub>1</sub> ⋃ *I*<sub>2</sub> \n",
    "* *O*<sub>*both*</sub> =<sub>df.</sub> *O*<sub>1</sub> ⋃ *O*<sub>2</sub>\n",
    "\n",
    "#### A-List\n",
    "*A*<sub>*X*</sub> =<sub>df.</sub> a selected set of words of set *X*, where *X*=*I* (ingroup) or *X*=*O* (outgroup); given some selection function ***Select()***, such that = *Select*(*X*) = *A*<sub>*X*</sub>. \n",
    "\n",
    "For example, let ***Select*<sub>*All*</sub>** be the function that selects all words of *X* (*Select*<sub>*All*</sub>(*X*) = *X*); and ***Select*<sub>*TopK*</sub>** be the function that selects the *K* most frequent word in *X*; e.g., *Select*<sub>*TopK*</sub>(*X*) = {*x*: *x* is among first *K* elements of *rank*(*X*, *f*)}, where *rank*(*X*, *f*) ranks the elements of *X* given some feature *f* (e.g., frequency).\n",
    "\n",
    "#### B-List\n",
    "*B*<sub>*X*</sub> =<sub>df.</sub> the set of words used to model the vector of dimensions **ingroup** (*X*=*I*) or **outgroup** (*X*=*O*). \n",
    "\n",
    "*B*<sub>*X*</sub> is related to *A*<sub>*X*</sub> via the function ***F***, such that *F*(*A*<sub>*X*</sub>) = *B*<sub>*X*</sub>. *F* should at least selects words in *A*<sub>*X*</sub> that also are in the vocabulary *V* of the `word2vec` model (matrix). A minimalist version of *F*, i.e. ***WINV***, does this and nothing else. More or less sophisticated version of *F* can be defined, based on additional steps to this minimal requirement (see below).\n",
    "\n",
    "### Methods:\n",
    "#### Really Naive (RN)\n",
    "* *B*<sub>*I*</sub> = *WINV*(*I*<sub>*both*</sub>)\n",
    "* *B*<sub>*O*</sub> = *WINV*(*O*<sub>*both*</sub>)\n",
    "\n",
    "*Problems:* \n",
    "* Overlap of dimensions (*I* and *O* share terms for some DWEs)\n",
    "* Misses morphologically related forms\n",
    "* Give equal attention to common and less common replacements\n",
    "\n",
    "#### Naive No Overlap (NNO)\n",
    "* *B*<sub>*I*</sub> = *WINV*({*w*: *w* in *I*<sub>*both*</sub> and *w* not in *O*<sub>*both*</sub>})\n",
    "* *B*<sub>*O*</sub> = *WINV*({*w*: *w* in *O*<sub>*both*</sub> and *w* not in *I*<sub>*both*</sub>})\n",
    "\n",
    "*Problems:* \n",
    "* The NO criterion can be too strong (note coding of replacements)!\n",
    "* Shares problems with RN\n",
    "\n",
    "#### Top 1 (T1), including NO\n",
    "\n",
    "\n",
    "#### Top 3 (T3), including NO\n",
    "\n",
    "\n",
    "#### Multi-Steps (MS) approaches\n",
    "\n",
    "For ***Select()***:\n",
    "\n",
    "|Feature  |Top|Threshold|\n",
    "|---------|---|---------|\n",
    "|Frequency|*a*|*b*      |\n",
    "|Keyness  |*c*|*d*      |\n",
    "\n",
    "\n",
    "For ***F()***:\n",
    "for *w* in *A*:\n",
    "* Identify lemma (or stem) *s* of *w*\n",
    "* (split compounds?)\n",
    "* Identify inflectional paradigm *G* for *s*; *G* = {wordform *w*: *w* is a inflectional varaint of *s*}\n",
    "* *G*<sup>*WINV*</sup> = *WINV*(*G*)\n",
    "* *C* = *FREQ*(*G*<sup>*WINV*</sup>)\n",
    "\n",
    "Next, three options:\n",
    "* Ignorant: ignore *C*, *β* = *G*<sup>*WINV*</sup>; *B* = *β*<sub>1</sub> ⋃ ... *β*<sub>*n*</sub> (*n* is the length of *A*)\n",
    "* Threshold: *β* = {*w*: *C*(*w*) < *T*} where T is some threshold \n",
    "* Weigthing: Use *C* as weights in building dimension\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "HOW SIMILAR ARE WORDS WITHIN AN INFLECTIONAL PARADIGM? ---> ignore outliers - or keep them!?\n",
    "\n",
    "#### Manual!\n",
    "...\n",
    "\n",
    "\n",
    "## 2. AVG BERT\n",
    "### Scope\n",
    "Complete or partial? I.e., all `meaning = 1` or just a selection? (a question similar to determine *L* above)\n",
    "\n",
    "### Out of vocabulary\n",
    "Will out of vocabulary be a problem for a non-domain adapted BERT? Will the tokenization of BERT \"take care\" of this? I.e., given the short \"sentences\" in replacements, will BERT be able to differentiate in/outgroup dimensions? This is the question of Hertzberg (et al.). And the answer was \"yes\" (but note fine-tuned vs. not fine-tuned). \n",
    "\n",
    "## 3. CLT BERT\n",
    "The same questions of scope and OOV as for AVG BERT applies for CLT BERT. Additional options for how to relate in/outgroup dimensions to clusters include:\n",
    "* Use the centroids of clusters\n",
    "* Use jaccard similarity with top terms of clusters, given some weigthed frequency distribution; conider, for example, class-based TF-IDF. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about *hjälpa på plats*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.nltk.org/api/nltk.stem.SnowballStemmer.html?highlight=stopwords\n",
    "from nltk.stem import SnowballStemmer # See which languages are supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join(SnowballStemmer.languages)) \n",
    "# arabic danish dutch english finnish french german hungarian\n",
    "# italian norwegian porter portuguese romanian russian\n",
    "# spanish swedish\n",
    "stemmer = SnowballStemmer(\"german\")\n",
    "stemmer.stem(\"Autobahnen\") # Stem a word\n",
    "# 'autobahn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\" \".join(SnowballStemmer.languages)) \n",
    "# arabic danish dutch english finnish french german hungarian\n",
    "# italian norwegian porter portuguese romanian russian\n",
    "# spanish swedish\n",
    "stemmer = SnowballStemmer(\"swedish\")\n",
    "stemmer.stem(\"deporterar\") # Stem a word\n",
    "# 'autobahn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
    "doc = nlp('This is a test sentence for stanza. This is another sentence.')\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')\n",
    "doc = nlp('Barack Obama was born in Hawaii.')\n",
    "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 09:16:28 INFO: Loading these models for language: sv (Swedish):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | talbanken |\n",
      "| pos       | talbanken |\n",
      "| lemma     | talbanken |\n",
      "=========================\n",
      "\n",
      "2023-11-15 09:16:28 INFO: Use device: cpu\n",
      "2023-11-15 09:16:28 INFO: Loading: tokenize\n",
      "2023-11-15 09:16:29 INFO: Loading: pos\n",
      "2023-11-15 09:16:30 INFO: Loading: lemma\n",
      "2023-11-15 09:16:30 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "nlp = stanza.Pipeline(lang='sv', processors='tokenize,pos,lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: invandrarungdomar \tlemma: invandrarungdom\n"
     ]
    }
   ],
   "source": [
    "#nlp = stanza.Pipeline(lang='sv', processors='tokenize,pos,lemma')\n",
    "doc = nlp(\"invandrarungdomar\")\n",
    "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')\n",
    "# doc = nlp('This is a test sentence for stanza. This is another sentence.')\n",
    "# for i, sentence in enumerate(doc.sentences):\n",
    "#     print(f'====== Sentence {i+1} tokens =======')\n",
    "#     print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'invandrarungdom'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sentences[0].words[0].lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(w):\n",
    "    doc = nlp(w)\n",
    "    return doc.sentences[0].words[0].lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bana'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer(\"banan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'banan'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer(\"bananen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kluckelimuckare'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer(\"kluckelimuckare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CITATION',\n",
       " 'List',\n",
       " 'UNIMORPH_DIR',\n",
       " 'UNIMORPH_DIR_',\n",
       " 'USERHOME',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'analyze_word',\n",
       " 'argparse',\n",
       " 'download_unimorph',\n",
       " 'get_list_of_datasets',\n",
       " 'inflect_word',\n",
       " 'is_empty',\n",
       " 'load_dataset',\n",
       " 'logging',\n",
       " 'main',\n",
       " 'not_loaded',\n",
       " 'os',\n",
       " 'parse_args',\n",
       " 'pathlib',\n",
       " 'pd',\n",
       " 'subprocess',\n",
       " 'sys']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unimorph\n",
    "dir(unimorph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "äter --> äta\n",
      "äta\täta\tV;NFIN;ACT\n",
      "äta\tätas\tV;NFIN;PASS\n",
      "äta\tätit\tV;V.CVB;ACT\n",
      "äta\tätits\tV;V.CVB;PASS\n",
      "äta\tät\tV;IMP;ACT\n",
      "äta\täter\tV;IND;SG;ACT;PRS\n",
      "äta\tåt\tV;IND;SG;ACT;PST\n",
      "äta\täts\tV;IND;SG;PASS;PRS\n",
      "äta\tätes\tV;IND;SG;PASS;PRS\n",
      "äta\tåts\tV;IND;SG;PASS;PST\n",
      "äta\täta\tV;IND;PL;ACT;PRS\n",
      "äta\tåto\tV;IND;PL;ACT;PST\n",
      "äta\tätas\tV;IND;PL;PASS;PRS\n",
      "äta\tåtos\tV;IND;PL;PASS;PST\n",
      "äta\täte\tV;SBJV;ACT;PRS\n",
      "äta\tåte\tV;SBJV;ACT;PST\n",
      "äta\tätes\tV;SBJV;PASS;PRS\n",
      "äta\tåtes\tV;SBJV;PASS;PST\n",
      "äta\tätande\tV;V.PTCP;PRS\n",
      "äta\täten\tV;V.PTCP;PST\n",
      "\n",
      "elit --> elit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in [\"äter\", \"elit\"]:\n",
    "    doc = nlp(s)\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            print(word.text, \"-->\", word.lemma)\n",
    "            print(unimorph.inflect_word(word.lemma, lang=\"swe\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mata\\tmata\\tV;NFIN;ACT\\nmata\\tmatas\\tV;NFIN;PASS\\nmata\\tmatat\\tV;V.CVB;ACT\\nmata\\tmatats\\tV;V.CVB;PASS\\nmata\\tmata\\tV;IMP;ACT\\nmata\\tmatar\\tV;IND;SG;ACT;PRS\\nmata\\tmatade\\tV;IND;SG;ACT;PST\\nmata\\tmatas\\tV;IND;SG;PASS;PRS\\nmata\\tmatades\\tV;IND;SG;PASS;PST\\nmata\\tmata\\tV;IND;PL;ACT;PRS\\nmata\\tmatade\\tV;IND;PL;ACT;PST\\nmata\\tmatas\\tV;IND;PL;PASS;PRS\\nmata\\tmatades\\tV;IND;PL;PASS;PST\\nmata\\tmate\\tV;SBJV;ACT;PRS\\nmata\\tmatade\\tV;SBJV;ACT;PST\\nmata\\tmates\\tV;SBJV;PASS;PRS\\nmata\\tmatades\\tV;SBJV;PASS;PST\\nmata\\tmatande\\tV;V.PTCP;PRS\\nmata\\tmatad\\tV;V.PTCP;PST\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unimorph.inflect_word(\"mata\", lang=\"swe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mata',\n",
       " 'matas',\n",
       " 'matat',\n",
       " 'matats',\n",
       " 'mata',\n",
       " 'matar',\n",
       " 'matade',\n",
       " 'matas',\n",
       " 'matades',\n",
       " 'mata',\n",
       " 'matade',\n",
       " 'matas',\n",
       " 'matades',\n",
       " 'mate',\n",
       " 'matade',\n",
       " 'mates',\n",
       " 'matades',\n",
       " 'matande',\n",
       " 'matad']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[line.split(\"\\t\")[1] for line in unimorph.inflect_word(\"mata\", lang=\"swe\").split(\"\\n\") if line != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "lemma = \"mat\"\n",
    "wfs = [line.split(\"\\t\")[1] for line in unimorph.inflect_word(lemma, lang=\"swe\").split(\"\\n\") if line != \"\"]\n",
    "print(wfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.cs.cmu.edu/~aanastas/software/unimorph_inflect.html\n",
    "# https://pypi.org/project/unimorph/\n",
    "# https://unimorph.github.io/\n",
    "# NOT GOOD! Rule based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame([wf.split(\"\\t\") for wf in unimorph.inflect_word(\"invandring\", lang=\"swe\").split(\"\\n\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unimorph.inflect_word(\"invandrarungdomar\", lang=\"swe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unimorph.analyze_word(\"invandring\", lang=\"swe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "#from util import load_metric, read_util\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "nlp = stanza.Pipeline(lang='sv', processors='tokenize,pos,lemma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(\"../../data/utils/stopwords-sv.txt\")) as f:\n",
    "    stopwords = [sw.replace(\"\\n\", \"\") for sw in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = Path(\"/home/max/Documents/research/replacement_data/panel_wide_onlyreplace.csv\")\n",
    "replacement_data = pd.read_csv(path_to_data, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacement_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacement_data = replacement_data.drop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacement_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df= df.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "replacement_data = replacement_data.applymap(lambda s: s.lower() if type(s) == str else s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(w.split(\"_\")[0] for w in replacement_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_data.loc[replacement_data['forortsgang_w1_C'] == 1, \"forortsgang_text_w1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect(\n",
    "    df,            # Replacement Dataframe\n",
    "    dwe,           # Dog Whistle Expression\n",
    "    meaning,       # 1 for ingroup, 2 for outgroup\n",
    "    phase,         # 1 for first phase of data collection, 2 for second phase\n",
    "    sw = None,     # stopwords\n",
    "    punct = None,  # remove punctuations\n",
    "    verbose = True,\n",
    "    multi = False, # Keep the multi-word units of the replacements\n",
    "    rel_freq = False # use relative frequncies freq / no. of documents\n",
    "):\n",
    "    \n",
    "    counter = Counter()\n",
    "    \n",
    "    if type(df) == pd.DataFrame:\n",
    "        column = df.loc[df[f\"{dwe}_w{phase}_C\"] == meaning, f\"{dwe}_text_w{phase}\"]\n",
    "    else:\n",
    "        column = df\n",
    "    \n",
    "    for x in column:\n",
    "        if punct != None:\n",
    "            for p in punct:\n",
    "                x = x.replace(p, \"\")\n",
    "        x = x.split()\n",
    "        if sw != None:\n",
    "            x = [w for w in x if w not in sw]\n",
    "        \n",
    "        if multi:\n",
    "            x = [\"_\".join(x)]\n",
    "        \n",
    "        counter.update(set(x)) # Obs. terms are only counted once per \"document\"\n",
    "    \n",
    "    if rel_freq:\n",
    "        counter = Counter({w: c/len(column) for w,c in counter.items()})\n",
    "        \n",
    "    if verbose:\n",
    "        for w, f in sorted(counter.items(), key = lambda x: x[1], reverse = True)[:15]:\n",
    "            print(f\"{w:<30}{f}\")\n",
    "        print(\"-----------------------\")\n",
    "        print(\"Total no. of types:\", len(counter))\n",
    "    \n",
    "    \n",
    "\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = inspect(replacement_data,\"forortsgang\",1,1,sw=stopwords,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sum(x.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(x.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x) # Number of words, length of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_data[replacement_data[\"forortsgang_w1_C\"] == 1].shape[0] # Number of \"documents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyness(trg, ref, min_frq = 3, verbose = True): # Consider metric\n",
    "    \n",
    "    d = dict()\n",
    "    \n",
    "#     trg_tot = sum(trg.values())\n",
    "#     ref_tot = sum(ref.values())\n",
    "    trg_tot = len(trg)\n",
    "    ref_tot = len(ref)\n",
    "    \n",
    "    for w in trg.keys():\n",
    "        if trg[w] < min_frq:\n",
    "            continue\n",
    "        if w in ref:\n",
    "            d[w] = (trg[w] / trg_tot) / (ref[w] / ref_tot) # Odds Ratio (OR)\n",
    "        else:\n",
    "            d[w] = np.inf\n",
    "    \n",
    "    if verbose:\n",
    "        for word, trg_freq, keyness  in sorted([(w, trg[w], k) for w, k in d.items()], key = lambda x: x[1], reverse = True)[:20]:\n",
    "            if word in ref:\n",
    "                ref_freq = ref[word]\n",
    "            else:\n",
    "                ref_freq = 0\n",
    "            print(f\"{word:<30}{trg_freq:<4}{(trg_freq/trg_tot):<6.3f}{ref_freq:<4}{(ref_freq/ref_tot):<6.3f}{keyness:.4}\")        \n",
    "    \n",
    "    return d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_A(\n",
    "    df,            # Replacement Dataframe\n",
    "    dwe,           # Dog Whistle Expression\n",
    "    phase,         # 1 for first phase of data collection, 2 for second phase, \"both\" for both\n",
    "    sw = None,     # stopwords\n",
    "    punct = None,  # remove punctuations\n",
    "    k = None,\n",
    "    min_freq = None,\n",
    "    min_OR = None,\n",
    "    empty_intersect = False\n",
    "):\n",
    "    \n",
    "    if type(k) == tuple:\n",
    "        k_in, k_out = k\n",
    "    else:\n",
    "        k_in  = k\n",
    "        k_out = k\n",
    "    if type(min_freq) == tuple:\n",
    "        min_freq_in, min_freq_out = min_freq\n",
    "    else:\n",
    "        min_freq_in  = min_freq\n",
    "        min_freq_out = min_freq\n",
    "    if type(min_OR) == tuple:\n",
    "        min_OR_in, min_OR_out = min_OR\n",
    "    else:\n",
    "        min_OR_in  = min_OR\n",
    "        min_OR_out = min_OR\n",
    "    \n",
    "    if phase == \"both\":\n",
    "        x = pd.concat([\n",
    "            df.loc[df[f\"{dwe}_w{1}_C\"] == 1, f\"{dwe}_text_w{1}\"],\n",
    "            df.loc[df[f\"{dwe}_w{2}_C\"] == 1, f\"{dwe}_text_w{2}\"]\n",
    "        ]).to_list()\n",
    "                \n",
    "        y = pd.concat([\n",
    "            df.loc[df[f\"{dwe}_w{1}_C\"] == 2, f\"{dwe}_text_w{1}\"],\n",
    "            df.loc[df[f\"{dwe}_w{2}_C\"] == 2, f\"{dwe}_text_w{2}\"]\n",
    "        ]).to_list()\n",
    "\n",
    "        ingroup = inspect(x, dwe, None, None, sw, punct, verbose = False, rel_freq = True)\n",
    "        outgroup = inspect(y, dwe, None, None, sw, punct, verbose = False, rel_freq = True)\n",
    "#         _ = inspect(x, dwe, None, None, sw, punct, multi = True)   \n",
    "#         _ = inspect(y, dwe, None, None, sw, punct, multi = True)   \n",
    "\n",
    "        keyness_in2out = keyness(ingroup, outgroup, verbose = False, min_frq = -1)\n",
    "        keyness_out2in = keyness(outgroup, ingroup, verbose = False, min_frq = -1)\n",
    "        \n",
    "    else:    \n",
    "    \n",
    "        ingroup = inspect(df, dwe, 1, phase, sw, punct, verbose = False, rel_freq = True)\n",
    "        outgroup = inspect(df, dwe, 2, phase, sw, punct, verbose = False, rel_freq = True)\n",
    "#         _ = inspect(df, dwe, 1, phase, sw, punct, multi = True)\n",
    "#         _ = inspect(df, dwe, 2, phase, sw, punct, multi = True)\n",
    "        keyness_in2out = keyness(ingroup, outgroup, verbose = False, min_frq = -1)\n",
    "        keyness_out2in = keyness(outgroup, ingroup, verbose = False, min_frq = -1)\n",
    "    \n",
    "    A_in  = [w for w in ingroup.keys()]\n",
    "    A_out = [w for w in outgroup.keys()]\n",
    "    #print(A_out)\n",
    "    \n",
    "    if empty_intersect:\n",
    "        A_in  = [w for w in A_in if w not in outgroup.keys()]\n",
    "        A_out = [w for w in A_out if w not in ingroup.keys()]\n",
    "        \n",
    "    if min_freq != None:\n",
    "        A_in  = [w for w in A_in if ingroup[w] >= min_freq_in]\n",
    "        A_out = [w for w in A_out if outgroup[w] >= min_freq_out]\n",
    "    \n",
    "    #print(A_out)\n",
    "    \n",
    "    if min_OR != None:\n",
    "        A_in  = [w for w in A_in if keyness_in2out[w] >= min_OR_in]\n",
    "        A_out = [w for w in A_out if keyness_out2in[w] >= min_OR_out] # too strict to have the same threshold for both\n",
    "        \n",
    "    #print(A_out)    \n",
    "    \n",
    "    if k != None:\n",
    "        A_in  = [w for w,_ in sorted(ingroup.items(), key = lambda x: x[1], reverse = True) if w in A_in][:k_in]\n",
    "        A_out = [w for w,_ in sorted(outgroup.items(), key = lambda x: x[1], reverse = True) if w in A_out][:k_out]\n",
    "    \n",
    "    \n",
    "    return A_in, A_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(word):\n",
    "    \n",
    "    doc = nlp(word)\n",
    "    \n",
    "    lemma = doc.sentences[0].words[0].lemma\n",
    "    \n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"äter\")\n",
    "doc.sentences[0].words[0].lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for s in [\"äter\", \"maten\"]:\n",
    "#     doc = nlp(s)\n",
    "#     for sent in doc.sentences:\n",
    "#         for word in sent.words:\n",
    "#             print(word.text, \"-->\", word.lemma)\n",
    "#             print(unimorph.inflect_word(word.lemma, lang=\"swe\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_B_prim(A):\n",
    "    \n",
    "    if lemmatize:\n",
    "        A = [lemmatizer(w) for w in A]\n",
    "    if realize:\n",
    "        pass\n",
    "\n",
    "        \n",
    "    return B_prim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_B(B_prim, vocab, wv, min_freq, exclude_marginal, margin):\n",
    "    \n",
    "    B = [w for w in B_prim if w in wv.key_to_index and w in vocab]\n",
    "    \n",
    "    if min_freq:\n",
    "        pass\n",
    "    if exclude_marginal:\n",
    "        pass\n",
    "    \n",
    "#     B.sort()\n",
    "    \n",
    "    return B\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def other(df, dwe, meaning, phase, criteria, verbose = True):\n",
    "    \n",
    "    oth = []\n",
    "    \n",
    "    if type(df) == pd.DataFrame:\n",
    "        column = df.loc[df[f\"{dwe}_w{phase}_C\"] == meaning, f\"{dwe}_text_w{phase}\"]\n",
    "    else:\n",
    "        column = df\n",
    "\n",
    "    for x in column:\n",
    "        keep = True\n",
    "        for criterion in criteria:\n",
    "            if criterion in x:\n",
    "                keep = False\n",
    "                continue\n",
    "        if keep:\n",
    "            oth.append(x)\n",
    "    \n",
    "    if verbose:\n",
    "        for x in oth:\n",
    "            print(x)\n",
    "    \n",
    "    return oth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_all(\n",
    "    df,            # Replacement Dataframe\n",
    "    dwe,           # Dog Whistle Expression\n",
    "    phase,         # 1 for first phase of data collection, 2 for second phase\n",
    "    sw = None,     # stopwords\n",
    "    punct = None,  # remove punctuations\n",
    "):\n",
    "    \n",
    "    if phase == \"both\":\n",
    "        print(\"Most frequent\")\n",
    "        print(\"=============\")        \n",
    "        \n",
    "        x = pd.concat([\n",
    "            df.loc[df[f\"{dwe}_w{1}_C\"] == 1, f\"{dwe}_text_w{1}\"],\n",
    "            df.loc[df[f\"{dwe}_w{2}_C\"] == 1, f\"{dwe}_text_w{2}\"]\n",
    "        ]).to_list()\n",
    "        \n",
    "                \n",
    "        y = pd.concat([\n",
    "            df.loc[df[f\"{dwe}_w{1}_C\"] == 2, f\"{dwe}_text_w{1}\"],\n",
    "            df.loc[df[f\"{dwe}_w{2}_C\"] == 2, f\"{dwe}_text_w{2}\"]\n",
    "        ]).to_list()\n",
    "\n",
    "        print(\"In-group:\")\n",
    "        print(\"(a) Split\")\n",
    "        ingroup = inspect(x, dwe, None, None, sw, punct)\n",
    "        print()\n",
    "        print(\"(b) Keep phrases\")\n",
    "        _ = inspect(x, dwe, None, None, sw, punct, multi = True)   \n",
    "        print(\"Out-group:\")\n",
    "        print(\"(a) Split\")\n",
    "        outgroup = inspect(y, dwe, None, None, sw, punct)\n",
    "        print()\n",
    "        print(\"(b) Keep phrases\")\n",
    "        _ = inspect(y, dwe, None, None, sw, punct, multi = True)   \n",
    "\n",
    "        \n",
    "        print()\n",
    "        print(\"Keyness\")\n",
    "        print(\"=======\")\n",
    "        print(f\"(a) in-group --> out-group\")\n",
    "        _ = keyness(ingroup, outgroup)\n",
    "        print()\n",
    "        print(f\"(b) out-group --> in-group\")\n",
    "        _ = keyness(outgroup, ingroup)\n",
    "        print()        \n",
    "        \n",
    "    else:    \n",
    "    \n",
    "        print(\"Most frequent\")\n",
    "        print(\"=============\")\n",
    "        print(\"In-group:\")\n",
    "        print(\"(a) Split\")\n",
    "        ingroup = inspect(df, dwe, 1, phase, sw, punct)\n",
    "        print()\n",
    "        print(\"(b) Keep phrases\")\n",
    "        _ = inspect(df, dwe, 1, phase, sw, punct, multi = True)\n",
    "        print()\n",
    "        print(\"Out-group:\")\n",
    "        print(\"(a) Split\")\n",
    "        outgroup = inspect(df, dwe, 2, phase, sw, punct)\n",
    "        print()\n",
    "        print(\"(b) Keep phrases\")\n",
    "        _ = inspect(df, dwe, 2, phase, sw, punct, multi = True)\n",
    "        print()\n",
    "\n",
    "        print(\"Keyness\")\n",
    "        print(\"=======\")\n",
    "        print(\"(a) in-group --> out-group\")\n",
    "        _ = keyness(ingroup, outgroup)\n",
    "        print()\n",
    "\n",
    "        print(\"(b) out-group --> in-group\")\n",
    "        _ = keyness(outgroup, ingroup)\n",
    "        print()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect_all(replacement_data, \"forortsgang\", \"both\", stopwords, [\",\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_forms(base_or_paradigm, m_dir, f_dir, stop = None):\n",
    "    \"\"\" Collects word forms starting with `base`\n",
    "    \"\"\"\n",
    "    \n",
    "    summary = []\n",
    "    sim_matrices = {}\n",
    "    \n",
    "    for file in sorted(os.listdir(m_dir)):\n",
    "        \n",
    "        if not file.endswith(\".w2v\"):\n",
    "            continue\n",
    "        print(file)    \n",
    "        year = int(file.replace(\".w2v\", \"\"))\n",
    "        if stop != None and year > stop:\n",
    "            break\n",
    "            \n",
    "        wv = KeyedVectors.load_word2vec_format(models_at / file)\n",
    "        vocab = load_metric(vocabs_at / file.replace(\".w2v\", \".txt\"))\n",
    "        \n",
    "        print(len(wv), len(vocab))\n",
    "\n",
    "        if type(base_or_paradigm) == str:\n",
    "            g = [w for w in wv.key_to_index if w.startswith(\"invandrare\")]\n",
    "        else:\n",
    "            assert type(base_or_paradigm) == list, \"`base_or_paradigm` is neither string nor list\"\n",
    "            g = [w for w in base_or_paradigm if w in wv.key_to_index]\n",
    "        g = [w if w in vocab else print(w, \"not in vocab\") for w in g]\n",
    "        g = [w for w in g if w != None]\n",
    "        g.sort()\n",
    "    #     print(g)\n",
    "\n",
    "        \n",
    "        c = [(w, int(vocab[w])) for w in g]\n",
    "        summary.append(\n",
    "            {\n",
    "            \"year\": year, \n",
    "            \"n\": len(g), \n",
    "            \"forms\": g, \n",
    "            \"counts\": c,\n",
    "            \"forms_str\": \", \".join([f\"{w} {n}\" for w, n in [(w, vocab[w]) for w in g]])\n",
    "            }\n",
    "        )\n",
    "\n",
    "        vectors = [wv[w] for w in g]\n",
    "\n",
    "        sim_matrix = cosine_similarity(vectors)\n",
    "\n",
    "        sim_matrices[year] = pd.DataFrame(sim_matrix, index=g, columns=g)\n",
    "    \n",
    "    \n",
    "    return summary, sim_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in combinations(np.array([[1,2,3,4],[4,5,7,1],[4,2,8,7]]), 2):\n",
    "    print(cosine_similarity([x,y])[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[1,2,3], [1,2,3]]).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_comparison(vectors):\n",
    "    \n",
    "    s = []\n",
    "    \n",
    "    for x, y in combinations(vectors, 2):\n",
    "        similarity = cosine_similarity([x,y])[0][1] # returns a matrix\n",
    "        s.append(similarity)\n",
    "    \n",
    "    s = np.array(s)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(dwe, repl_data, m_dir, f_dir):\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for dim in [1, 2]:\n",
    "        A = \n",
    "        B_prim = \n",
    "\n",
    "        for file in sorted(os.listdir(m_dir)):\n",
    "            \n",
    "            year = int(file.replace(\".txt\", \"\"))\n",
    "\n",
    "            if not file.endswith(\".w2v\"):\n",
    "                continue            \n",
    "            \n",
    "            wv = KeyedVectors.load_word2vec_format(models_at / file)\n",
    "            \n",
    "            vocab = load_metric(vocabs_at / file.replace(\".w2v\", \".txt\"))\n",
    "            \n",
    "            dwe_vec = wv[dwe]\n",
    "            \n",
    "            B = build_B(B_prim, vocab, wv, min_freq, exclude_marginal, margin)\n",
    "\n",
    "            vectors = np.array([wv[w] for w in B])\n",
    "\n",
    "            avg_vec = vectors.mean(axis = 0)\n",
    "            \n",
    "            distance = cosine_similarity([dwe_vec, avg_vec])\n",
    "\n",
    "            series = pairwise_comparison(vectors)\n",
    "            \n",
    "            pairwise_mean = series.mean()\n",
    "            \n",
    "            dim_term = \"ingroup\" if dim == 1 else \"outgroup\"\n",
    "            \n",
    "            results[dim_term] = {}\n",
    "            \n",
    "    \n",
    "    for year in years:\n",
    "        t, p = ttest(series_in, series_out)\n",
    "    \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim in [1, 2]:\n",
    "    dim_term = \"ingroup\" if dim == 1 else \"outgroup\"\n",
    "    print(dim_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity([[1,2,3], [2,3,4], [4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_at = Path(\"/home/max/Results/fb_pol-yearly-rad3/models\")\n",
    "vocabs_at = Path(\"/home/max/Corpora/flashback-pol-time/yearly/fb-pt-radical3/vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary, sim_mtrx = collect_forms(\"invandrare\", models_at, vocabs_at, stop = 2005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in sim_mtrx.keys():\n",
    "    print(year)\n",
    "    df = sim_mtrx[year]\n",
    "    print(df)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [np.array([1.2, 2.3]), np.array([5.6, 4.2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    \"mamma\": 1,\n",
    "    \"mamman\": 3,\n",
    "    \"mu\": 100,\n",
    "    \"kråka\": 1,\n",
    "    \"kråkor\":20\n",
    "}\n",
    "\n",
    "\n",
    "Bigt = [(\"mamma\", \"mamman\"), (\"mu\",), (\"kråka\", \"kråkan\", \"kråkor\")]\n",
    "voc_B = {lexeme[0].upper(): {w: vocab[w] for w in lexeme if w in vocab} for lexeme in Bigt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_B[\"MAMMA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = {lexeme: {w: (voc_B[lexeme][w]/sum(voc_B[lexeme].values())) for w in voc_B[lexeme]} for lexeme in voc_B}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(voc_B[\"MAMMA\"].items(), key = lambda x: x[1], reverse=True)[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.listdir(models_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_metric(vocabs_at / \"2000.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"2004.w2v\"\n",
    "wv = KeyedVectors.load_word2vec_format(models_at / model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(wv[\"mamma\"]) == np.ndarray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wv.get_vector(\"sverige\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wv[\"sverige\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in sorted(os.listdir(models_at)):\n",
    "    if not model.endswith(\".w2v\"):\n",
    "        continue\n",
    "    wv = KeyedVectors.load_word2vec_format(models_at / model)\n",
    "    g = [w for w in wv.key_to_index if w.startswith(\"invandrare\")]\n",
    "    g.sort()\n",
    "#     print(g)\n",
    "    print(model[:4], len(g), \", \".join(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in sorted(os.listdir(models_at)):\n",
    "    if not model.endswith(\".w2v\"):\n",
    "        continue\n",
    "    wv = KeyedVectors.load_word2vec_format(models_at / model)\n",
    "    g = [w for w in wv.key_to_index if w.startswith(\"deportera\")]\n",
    "    g.sort()\n",
    "#     print(g)\n",
    "    print(model[:4], len(g), \", \".join(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in sorted(os.listdir(models_at)):\n",
    "    if not model.endswith(\".w2v\"):\n",
    "        continue\n",
    "    wv = KeyedVectors.load_word2vec_format(models_at / model)\n",
    "    g = [w for w in wv.key_to_index if w.startswith(\"elitist\")]\n",
    "    g.sort()\n",
    "#     print(g)\n",
    "    print(model[:4], len(g), \", \".join(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wv[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"sverige\" in wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wv[\"sverige\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[w for w in wv.key_to_index if w.startswith(\"invandr\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_mean_vector(keys, weights=None, pre_normalize=True, post_normalize=False, ignore_missing=True)\n",
    "wv.get_mean_vector(['invandrare', 'invandrarna'], weights=[0.7, 0.3], pre_normalize=True, post_normalize=False, ignore_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.similarity('invandrare', 'invandrarna')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.n_similarity(['invandrare'], ['invandrarna', 'invandrarnas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.similarity('invandrare', 'invandrargäng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.similarity('invandrare', 'människor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar(\"V1_berika\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "igt = [\n",
    "    \"förstöra\",\n",
    "    \"utnyttja\",\n",
    "    \"förpesta\",\n",
    "    \"försämra\", \n",
    "    \"brott\",\n",
    "    \"fördärva\"\n",
    "    ]\n",
    "ogt = [\n",
    "    \"förbättra\",\n",
    "    \"tillföra\",\n",
    "    \"bidra\",\n",
    "    \"förgylla\",\n",
    "    \"nytta\",\n",
    "    \"hjälpa\", \n",
    "    \"främja\",\n",
    "    \"bättre\",\n",
    "    \"utveckla\",\n",
    "    \"stärka\",\n",
    "    \"gynna\",\n",
    "    \"förhöja\",\n",
    "    \"positiv\"\n",
    "    ]\n",
    "wv.n_similarity([\"V1_berika\"], igt), wv.n_similarity([\"V1_berika\"], ogt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Association metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def association(\n",
    "    trg,        # Targets (list)\n",
    "    igt,        # In-group terms\n",
    "    ogt,        # Out-group terms\n",
    "    kwv,        # Keyed word vector (`gensim.models.KeyedVectors`)\n",
    "    method = \"centroid\", # \"centroid\" for similarity of averaged vector (centroid); \n",
    "                         # \"mean\" for mean of similarity of each comparison\n",
    "    weigths_trg = None,  # weigths of trg\n",
    "    weigths_igt = None,  # weigths of igt\n",
    "    weigths_ogt = None,  # weigths of ogt\n",
    "):\n",
    "    \n",
    "    if method == \"centroid\":\n",
    "        \n",
    "        trg_vec = kwv.get_mean_vector(trg, weights=weigths_trg, pre_normalize=True, post_normalize=False, ignore_missing=True)\n",
    "        igt_vec = kwv.get_mean_vector(igt, weights=weigths_igt, pre_normalize=True, post_normalize=False, ignore_missing=True)\n",
    "        ogt_vec = kwv.get_mean_vector(ogt, weights=weigths_ogt, pre_normalize=True, post_normalize=False, ignore_missing=True)\n",
    "        \n",
    "        igs = cosine_similarity([trg_vec, igt_vec])[0][1] # returns a matrix! must be indexed\n",
    "        ogs = cosine_similarity([trg_vec, ogt_vec])[0][1]\n",
    "        \n",
    "        t = None\n",
    "        p = None\n",
    "\n",
    "    if method == \"mean\":\n",
    "        \n",
    "        igscores = []\n",
    "        ogscores = []\n",
    "        \n",
    "        for trg_i in trg:      # What about OOV items? What do those calculations return?\n",
    "            for igt_i in igt:\n",
    "                ig_scores.append(kwv.similarity(trg_i, igt_i))\n",
    "        \n",
    "        for trg_i in trg:\n",
    "            for ogt_i in ogt:\n",
    "                og_scores.append(kwv.similarity(trg_i, ogt_i))\n",
    "        \n",
    "        igs = np.mean(igscores) # list to numpy array? axis = ? Check!\n",
    "        ogs = np.mean(ogscores)\n",
    "        \n",
    "        t, p, df = ttest_ind(igscores, ogscores) # it is not perfectly clear that a t-test is a good idea here\n",
    "        \n",
    "    return igs, ogs, t, p\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Test: återvandring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity([[1,2,2], [2,33,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igt = [\n",
    "        \"utvisa\", \n",
    "        \"skicka\", \n",
    "        \"deportering\", \n",
    "        \"tvångsförflyttning\", \n",
    "        \"kasta\", \n",
    "        \"sända\", \n",
    "        \"tvinga\", \n",
    "        \"slänga\", \n",
    "        \"avvisa\",\n",
    "        \"sparka\",\n",
    "        \"avhysa\"\n",
    "]\n",
    "ogt = [\n",
    "    \"återvända\",\n",
    "    \"återreas\",\n",
    "    \"repatriering\",\n",
    "    \"hemresa\",\n",
    "    \"hemåtvända\",\n",
    "    \"utvandring\",\n",
    "    \"flytta\"\n",
    "    ]\n",
    "trg = [\"N1_återvandring\"]\n",
    "for file in sorted([file for file in os.listdir(models_at) if file.endswith(\".w2v\")]):\n",
    "    \n",
    "    wv = KeyedVectors.load_word2vec_format(models_at / file)\n",
    "    \n",
    "    print(file, association(trg, igt, ogt, wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted({\"a\": 1, \"b\":20}.items(), key = lambda x: x[1], reverse=True)\n",
    "sorted({\"a\": 1, \"b\":20, \"c\":10}.items(), key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Förortsgäng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inspect_all(replacement_data, \"forortsgang\", \"both\", stopwords, [\",\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* keyness > 5\n",
    "\n",
    "Some considerations in coding:\n",
    "* In *O* when it should not:\n",
    "    * invandrargäng 9\n",
    "    * invandrare 12\n",
    "    * invandrarungdomar 2\n",
    "    * invandrarkillar 1\n",
    "    * utländsk 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_A(\n",
    "    replacement_data,            # Replacement Dataframe\n",
    "    \"forortsgang\",           # Dog Whistle Expression\n",
    "    \"both\",         # 1 for first phase of data collection, 2 for second phase, \"both\" for both\n",
    "    sw = stopwords,     # stopwords\n",
    "    punct = [\",\"],  # remove punctuations\n",
    "    k = 5,\n",
    "    min_freq = None,\n",
    "    min_OR = None,\n",
    "    empty_intersect = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "select_A(\n",
    "    replacement_data,            # Replacement Dataframe\n",
    "    \"forortsgang\",           # Dog Whistle Expression\n",
    "    \"both\",         # 1 for first phase of data collection, 2 for second phase, \"both\" for both\n",
    "    sw = stopwords,     # stopwords\n",
    "    punct = [\",\"],  # remove punctuations\n",
    "    k = None,\n",
    "    min_freq = 0.03,\n",
    "    min_OR = (10, 2),\n",
    "    empty_intersect = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inspect_all(replacement_data, \"forortsgang\", 1, stopwords, [\",\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ingroup terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "igt = [\n",
    "        \"invandr\", \n",
    "        \"blatt\", \n",
    "        \"utländsk\", \n",
    "        \"nysvensk\", \n",
    "        \"indvad\",     # Miss-spelling ...\n",
    "        \"invandar\", \n",
    "        \"invadra\", \n",
    "        \"innvandr\", \n",
    "        \"indvandr\"\n",
    "    ]\n",
    "_ = other(\n",
    "    df = replacement_data, \n",
    "    dwe = \"forortsgang\", \n",
    "    meaning = 1, \n",
    "    phase = 1, \n",
    "    criteria = igt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outgroup terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Hard! See above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_all(replacement_data, \"forortsgang\", 2, stopwords, [\",\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ingroup terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igt = [\n",
    "        \"invandr\", \n",
    "        \"blatt\", \n",
    "        \"babb\",\n",
    "        \"svartskall\",\n",
    "        \"neger\",\n",
    "        \"utlänning\", \n",
    "        \"utlandsfödd\", \n",
    "        \"ickesvensk\",\n",
    "        \"nysvensk\",\n",
    "        \"arab\",\n",
    "        \"utländsk\",\n",
    "        \"utomnordisk\",\n",
    "        \"utomeuropeisk\",\n",
    "        \"invanda\",        # Miss-spelling\n",
    "        \"invadr\",\n",
    "        \"ivandr\"\n",
    "    ] \n",
    "_ = other(\n",
    "    df = replacement_data, \n",
    "    dwe = \"forortsgang\", \n",
    "    meaning = 1, \n",
    "    phase = 2, \n",
    "    criteria = igt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outgroup terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Hard! See above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Återvandring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inspect_all(replacement_data, \"aterinvandring\", \"both\", sw = stopwords, punct = [\",\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "select_A(\n",
    "    replacement_data,            # Replacement Dataframe\n",
    "    \"aterinvandring\",           # Dog Whistle Expression\n",
    "    \"both\",         # 1 for first phase of data collection, 2 for second phase, \"both\" for both\n",
    "    sw = stopwords,     # stopwords\n",
    "    punct = [\",\"],  # remove punctuations\n",
    "    k = None,\n",
    "    min_freq = 0.03,\n",
    "    min_OR = (10, 2),\n",
    "    empty_intersect = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inspect_all(replacement_data, \"aterinvandring\", 1, sw = stopwords, punct = [\",\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ingroup terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "igt = [\n",
    "        \"utvis\", \n",
    "        \"skick\", \n",
    "        \"deport\", \n",
    "        \"tvångsförflyttning\", \n",
    "        \"kast\", \n",
    "        \"sänd\", \n",
    "        \"tvinga\", \n",
    "        \"släng\", \n",
    "        \"avvis\"\n",
    "    ]\n",
    "_ = other(\n",
    "    df = replacement_data, \n",
    "    dwe = \"aterinvandring\", \n",
    "    meaning = 1, \n",
    "    phase = 1, \n",
    "    criteria = igt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment:* wrong code? återvändande, tillbakaflytt, repatriering, hemvändande"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outgroup terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ogt = [\n",
    "    \"hemvänd\",\n",
    "    \"återvänd\",\n",
    "    \"återres\",\n",
    "    \"repatrier\",\n",
    "    \"hemres\",\n",
    "    \"hemåtvända\",\n",
    "    \"utvandring\",\n",
    "    \"flytt\"\n",
    "    ]\n",
    "_ = other(\n",
    "    df = replacement_data, \n",
    "    dwe = \"aterinvandring\", \n",
    "    meaning = 2, \n",
    "    phase = 1, \n",
    "    criteria = ogt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment:* wrong code? On several. utvisning, etnisk rensning, skicka tillbaka, avvisa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_all(replacement_data, \"aterinvandring\", 2, sw = stopwords, punct = [\",\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ingroup terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "igt = [\n",
    "        \"utvis\", \n",
    "        \"skick\", \n",
    "        \"deport\", \n",
    "        \"tvångsförflyttning\", \n",
    "        \"kast\", \n",
    "        \"sänd\", \n",
    "        \"tvinga\", \n",
    "        \"släng\", \n",
    "        \"avvis\",\n",
    "        \"spark\",\n",
    "        \"avhys\",\n",
    "        \"förvis\"\n",
    "    ]\n",
    "_ = other(\n",
    "    df = replacement_data, \n",
    "    dwe = \"aterinvandring\", \n",
    "    meaning = 1, \n",
    "    phase = 2, \n",
    "    criteria = igt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment:* wrong code? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outgroup terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ogt = [\n",
    "    \"hemvänd\",\n",
    "    \"återvänd\",\n",
    "    \"återres\",\n",
    "    \"repatrier\",\n",
    "    \"hemres\",\n",
    "    \"hemåtvända\",\n",
    "    \"utvandring\",\n",
    "    \"flytt\"\n",
    "    ]\n",
    "_ = other(\n",
    "    df = replacement_data, \n",
    "    dwe = \"aterinvandring\", \n",
    "    meaning = 2, \n",
    "    phase = 2, \n",
    "    criteria = ogt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment:* wrong code? förpassning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Berikar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inspect_all(replacement_data, \"berikar\", \"both\", sw = stopwords, punct = [\",\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_all(replacement_data, \"berikar\", 1, sw = stopwords, punct = [\",\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ingroup terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "igt = [\n",
    "    \"förstör\",\n",
    "    \"utnyttja\",\n",
    "    \"förpest\",\n",
    "    \"försämra\", \n",
    "    \"brott\",\n",
    "    \"fördärv\"\n",
    "    ]\n",
    "_ = other(\n",
    "    df = replacement_data, \n",
    "    dwe = \"berikar\", \n",
    "    meaning = 1, \n",
    "    phase = 1, \n",
    "    criteria = igt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outgroup terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ogt = [\n",
    "    \"förbättra\",\n",
    "    \"tillför\",\n",
    "    \"bidra\",\n",
    "    \"förgyll\",\n",
    "    \"nytta\",\n",
    "    \"hjälp\", \n",
    "    \"främja\",\n",
    "    \"bättre\",\n",
    "    \"utveckla\",\n",
    "    \"stärk\",\n",
    "    \"gynna\",\n",
    "    \"förhöj\",\n",
    "    \"positiv\"\n",
    "    ]\n",
    "_ = other(\n",
    "    df = replacement_data, \n",
    "    dwe = \"berikar\", \n",
    "    meaning = 2, \n",
    "    phase = 1, \n",
    "    criteria = ogt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_all(replacement_data, \"berikar\", 2, sw = stopwords, punct = [\",\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ingroup terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "igt = [\n",
    "    \"förstör\",\n",
    "    \"utnyttja\",\n",
    "    \"förpest\",\n",
    "    \"försämra\", \n",
    "    \"brott\",\n",
    "    \"fördärv\"\n",
    "    ]\n",
    "_ = other(\n",
    "    df = replacement_data, \n",
    "    dwe = \"berikar\", \n",
    "    meaning = 1, \n",
    "    phase = 1, \n",
    "    criteria = igt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outgroup terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ogt = [\n",
    "    \"förbättra\",\n",
    "    \"tillför\",\n",
    "    \"bidra\",\n",
    "    \"förgyll\",\n",
    "    \"nytta\",\n",
    "    \"hjälp\", \n",
    "    \"främja\",\n",
    "    \"bättre\",\n",
    "    \"utveckla\",\n",
    "    \"stärk\",\n",
    "    \"gynna\",\n",
    "    \"förhöj\", \n",
    "    \"positiv\"\n",
    "    ]\n",
    "_ = other(\n",
    "    df = replacement_data, \n",
    "    dwe = \"berikar\", \n",
    "    meaning = 2, \n",
    "    phase = 1, \n",
    "    criteria = ogt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Globalister"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_all(replacement_data, \"globalister\", \"both\", sw = stopwords, punct = [\",\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_all(replacement_data, \"globalister\", 1, sw = stopwords, punct = [\",\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ingroup terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "igt = [\n",
    "    \"judar\",\n",
    "    \"elit\"\n",
    "    ]\n",
    "_ = other(\n",
    "    df = replacement_data, \n",
    "    dwe = \"globalister\", \n",
    "    meaning = 1, \n",
    "    phase = 1, \n",
    "    criteria = igt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outgroup terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ogt = [\n",
    "    \"internationell\" # ?\n",
    "\n",
    "    ]\n",
    "_ = other(\n",
    "    df = replacement_data, \n",
    "    dwe = \"globalister\", \n",
    "    meaning = 2, \n",
    "    phase = 1, \n",
    "    criteria = ogt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_all(replacement_data, \"globalister\", 2, sw = stopwords, punct = [\",\", \".\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ingroup terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "igt = [\n",
    "    \"judar\",\n",
    "    \"judisk\",\n",
    "    \"soro\",\n",
    "    \"elit\"\n",
    "    ]\n",
    "_ = other(\n",
    "    df = replacement_data, \n",
    "    dwe = \"globalister\", \n",
    "    meaning = 1, \n",
    "    phase = 2, \n",
    "    criteria = igt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outgroup terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ogt = [\n",
    "    \"internationell\", # ?\n",
    "    \"värld\"\n",
    "\n",
    "    ]\n",
    "_ = other(\n",
    "    df = replacement_data, \n",
    "    dwe = \"globalister\", \n",
    "    meaning = 2, \n",
    "    phase = 2, \n",
    "    criteria = ogt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment:* Polysemous dogwhistle... \n",
    "* landsförrädare, landsförädare, icke-nationalister; \n",
    "* kapitaliser; \n",
    "* liberalister; \n",
    "* sossar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
