# Report
31-03-2023<br/>
A github repositiory is found here: https://github.com/mboholm/dw_change

## A note on technical issues
In the `gensim` library it is possible to train a skipgram model with weigths intialized from another model, i.e. a form of fine-tuning. In the code by Noble et al., this procedure is implemented with Gensim 3.8, but Gensim 3.8 is not really compatible with Python 3.10, which is the version of Python installed on the MLTGPU server. Therefore I did first train models with Gensim 4 on the server. However, some things are done differently in Gensim 3 and Gensim 4. For example, the procedure for fine-tuning in Gensim 3 is no longer supported in Gensim 4. I did implement the suggested workaround found [here](https://github.com/RaRE-technologies/gensim/issues/3094), which seemed to be working until I checked the results. I am not sure exactly why this fails, but I think it has to do with the vector that determines which words to be updated during training and which are to be left alone: the `vectors_lockf` feature. This is a vector of the same length as the vocabulary to be trained. It has the value 1.0 for the position of the word to be updated and the value 0.0 for those words that should not. Despite apparantly being initialized with all 1s as it should, i.e. for the intersection of the vocabularies of the pre-trained model and the model to train, the results of the training looks as if the 1s are distrubted at random. Some words get updated, but others are not, even though the word is defined for both models.    

I did try to figure out a way to run Python 3.8, which is compatible with Gensim 3, on the server, but I did not succeed. I emailed support (R.A.) and asked if he could help, but have not yet got any answer. 

~~**Question** Is there a way of running Python 3.8 on the server?~~ (There seems to be a solution for this)

For now, I have trained the SGNS models on my laptop, which works fine, but it takes a lot of time (almost two days with the hyperparameters presented below) and it is not possible to use you computer for anything else meanwhile. 

## Overview
### Vectorization of language
As we have discussed, I have tested two different approaches to vectorize text:

* Skip gram with negative sampling (SGNS)
* Sentence-BERT

**Note:** We discussed that GloVe could be an option as well. I have not done this yet. There are exisiting implementations to train your own glove vectors, e.g. [Stanford NLP](https://github.com/stanfordnlp/glove), [`glove_python` (PyPi)](https://pypi.org/project/glove_python/), and [`glove` (PyPi)](https://pypi.org/project/glove/). However, how to get comparable vectors for temporal analysis might add an extra challenge when using GloVe. At least, at the moment, I have no clear idea how to do that. To be continued ...<br/> 

From N.H., I got three pre-trained sentence-BERT models, all trained on the corpus *Flashback-politik*:

* *nli* (trained on NLI  task)
* *sts* (trained on STS task)
* *sts_big* (also trained on STS task, but somehow bigger and with more epochs; I have to check the details here...)

In addition, I have added KB's sentence-BERT (in the Huggingface Transformers library is called by `'KBLab/sentence-bert-swedish-cased'`); it is not fine-tuned on Flashback data. 

### "Base" corpus
The "root" corpus for the work done so far is *Flashback-politik* from [Spraakbanken](https://spraakbanken.gu.se/resurser/flashback-politik). This is a huge XML file (21 GB compressed and almost 300 GB uncompressed). A "base" corpus was built by iterating over the XML file such that every example *x* at year *y* was saved to text file `y.txt`, e.g. `2001.txt` (see `data/read_large_xml.py` if interested). The data ranges from 2000 to 2022. 

**TABLE 1: Data**
|Year|No. ex.|No. tokens|
|--|--:|--:|
|2000|122161|1520519|
|2001|76244|987848|
|2002|81955|1082399|
|2003|50347|750244|
|2004|232512|3509437|
|2005|327599|4803985|
|2006|627859|9090457|
|2007|886365|12839124|
|2008|1586388|22773244|
|2009|1842743|26503014|
|2010|2617321|38637940|
|2011|2844107|42062337|
|2012|2649618|39722512|
|2013|2524632|37897889|
|2014|2562851|38101053|
|2015|2700569|39519520|
|2016|2620037|37281369|
|2017|2122799|30097615|
|2018|2487120|35059879|
|2019|2125177|29904477|
|2020|2323126|32086434|
|2021|2360910|32765906|
|2022|3615275|48356017|

### Temporal unit
Temporal analysis requires some partioning along the time axis. There are several possibilities. So far I have focused on a time series of consecutive years (i.e. 2000, 2001, 2002, ..., 2022). This is a natural starting point for studying short-term semantic drift, but for a time span of 23 years (22 consecutive transitions) analysis become somewhat "messy" (at the same time it is probably too limited to for more sophisticated forms of time series analysis). 

I have also considered four-year "time bins" starting from 2003, roughly mapping the periods of Swedish elections (2003-2006, 2007-2010, ..., 2019-2022). However, for technical reasons discussed above, the SGNS models I did train for the four-year time bins turned out to be useless. I have not yet trained new SGNS models for four-year time bins. Also note, four-year time bins can easily be implemented for the BERT models, but I have not yet done that. To be continued ... 

## Preprocessing
### General
In general data for experiments on SGNS and sentence-BERT have been preprocessed as follows:

* Remove URLs
* Remove emojis
* Remove numbers (this should probably not have been done for BERT experiments; I will re-consider)
* Remove punctuations (for BERT experiments I realize that this is a mistake; should be reconsidered)
* Lower case 

### Dog whistle terms (DWTs)
This work has so far focused on the following "base forms" of known-to-be Swedish DWTs: 

* "förortsgäng"
* "återvandr"
* "berika"
* "kulturberika"
* "ordning och reda i flyktingpolitiken"
* "globalist"
* "hjälpa på plats"

Clearly these "base forms" does not map to the theoretical notions of "base" or "root" in morphological analyis (the list even contain phrases). Nevertheless, we do observe that they are part of (great) morphological variability. Especially in Swedish (compared to English) the noun and adjective forms of these base forms have a rather extensive inflectional paradigms; for example:

* *globalist* (noun): "globalist", "globalisten", "globalister", "globalisterna", "globalisternas", "globalisters", "globalists", ...
* *globalistisk* (adjective): "globalistisk", "globalistiska", "globalistiske", "globalistiskt"

To this we can add compound forms. In Swedish orthography compounds are written without space, e.g. *globalistelit* (eng. globalist elite). 

Note that this morphological variability is not only a theoretical matter. There are thousands of forms in the corpus related to the DWT bases listed above. 

#### DWT lemmtization for SGNS experiments 
Given the variability of DTW forms, generalization is required for analysis. I have lemmatized the DWT forms, but other strategies are certainly possible (e.g., one can train SGNS models on the word forms and then take the centroid of the wordform vectors as the representation of the lexeme). Lemmatization of DWTs is carried out through regular expressions. The rules are defined in `data/utils/dwts.paradigm`. For example, consider a snippet of the rules for "berika":

```
berika -> N1 -> \bberikar(e|en|na|es|ens|nas)\b -> berikare
berika -> V1 -> \bberik(a|ar|ad|ade|at|ande|as|ats|ades)\b -> berika # incl. particip
```

In lemmatization a match of the regular expression in the third column is replaced by the POS in the second column + underscore + the term in the last column; e.g. "berikarna" will be replaced by "N1_berikare". <br/>

The rules above does not handle compound forms. For this another approach has been chosen:

1. The compound is split (insert space) into the DWT root and its second base; basically, English orthography is applied to Swedish (the Swedish word for this process is *särskrivning*)
2. The DWT base of the compound is lemmatized as above

For example, "globalisteliten" is replaced by "N1_globalist eliten". 

This preprocessing of data for the SGNS experiments is certainly not innocent, but should be given more thought. Other strategies can certainly be considered. However, the approach here attempts to handle the wide variability of DWTs in corpora. Considering the implementation of *särskrivning* in preprocessing, the rationale is to consider rigth-hand bases as part of the context of the DWT (much as would have been the case in English data). Note that DWT compounds where the DWT is the left hand component of the compound is not (yet) considered for preprocessing. For example, "mångkulturberika" is *not* lemmatized in the data. 

#### Preparing data for BERT experiments
*Note:* In the BERT experiments, DWTs where not lemmatized. Instead, examples for analysis where collected from the data such that:

1. Iterate through the exampels of the prepocessed corpus
2. For every example *x* at a particalar time unit *u* (e.g., years), considering the list of DWT "base forms" listed above, call it *F*: does the example contain a word that matches a base form *f* in *F*? If so, go to the next step; else: go to the next example.
3. Which DWT does the example contain? For exampele, "kulturberikare". Several matches of the example are identified, if present.
4. Save the following tuple to `y.txt` (e.g. `2000.txt`): (*lemmatzed DWT*, *No. of DWT matches*, *x*), for example: 
```
V1_berika	1	man utbyter erfarenheter och åsikter i syfte att berika sig och varandra
``` 

## Training and collecting data for semantic change
### SGNS
SGNS models and training were implemented through Gensim. SGNSs models have been trained for yearly separated data. (As noted above, the models that were in fact trained on data separated by four-year time bins were discarded. A new experment can be planned for this.) Data was preprocessed as described above, including DWT lemmatization and *särskrivning*. For every time unit *u* (*u* = year), a SGNS model `u.w2v` was trained on examples in `u.txt` of the data. Also, as implemented by Noble et al., for every transition (*u_i*, *u_j*), do: 

1. For every word *w* in both `u_i.w2v` and `u_j.w2v` record (i) the cosine similarity and (ii) the angular distance of *w*'s vectors
2. Train a set of *n* controls (*n* = 10), such that:

	1. Concatenate file `u_i.txt` and `u_j.txt` as `c.txt`
	2. Shuffle `c.txt`
	3. Split `c.txt` in half, resulting in `c1.txt` and `c2.txt`
	4. Train a SGNS model for each (`c1.w2v` and `c2.w2v`)
	5. For every word *w* in both `c1.w2v` and `c2.w2v` record (i) the cosine similarity and (ii) the angular distance of *w*'s vectors

Controls are used to calculate rectified change as defined in Noble et al. (see below).

#### Hyperparameters
* Minimum frequency of word = 10 (this is a very low number; more restricted selection of data in analysis is possible)
* Window size = 5 
* Epochs = 10 (this is a low number, but training takes a lot of time; with a higher numer, it would take even longer). The training algorithm also implements a stopping rule: if the average angular distance between the model of the current epoch and the model of the previous epoch reaches a threshold of 0.0001, then: stop training. No epoch of training reached this threshold.   
* No. of "noise words" (in negative sampling) = 5
* Negative sampling exponent = 0.75 
* Initial learning rate = 0.025

### Sentence BERT
Only pretrained models have been used to gather data for semantic change. The procedure is as follows:
#### Step 1: vextorize examples
As noted above, for the BERT experiments, we do not use the full corpus. We only consider examples where a DWT was identified. Procedure:

* for every DWT context, represented as (*u*, *dwt*, *c*), where *u* = time unit (e.g., year), *dwt* = generalized DWT (e.g. *N1_kulturberikare*), and *c* = context (i.e. the preprocessed example from the data):   
	* for every model *m* in the models *nli*, *sts*, *sts_big* and *KB* (discussed above):
		* encode *c* as vector *v*, save as (*u*, *dwt*, *v*); a (*dwt*, *v*) pair is saved to `u.txt` in directory `m`. 

##### Step 2
For every (*u*, *dwt*) pair (of every model *m*) we can calculate:
* The meaning of *dwt* at *u* (given *m*); here I represents *dwt*'s meaning at *u* as the *centroid* of all vectors *V* for *dwt* in *u*.
* The *"spread"* of *dwt* at *u*; i.e. the mean of all pairwise cosine similiarity calculations *v_i* and *v_j* in *V* (excluding the pair of a vector and itself; also note: *sim*(*v_i*, *v_j*) is considered equal to *sim*(v*v_i*, *v_j*) and is only considered once).

## Some preliminary results
### Generalized DWTs (GDWTs) in focus
Following the procedures above, the following generalised DWTs, **GDWTs**:

* A1_globalistisk
* N1_berikare
* N1_förortsgäng
* N1_globalist
* N1_kulturberikare
* N1_återvandring
* N2_återvandrare
* V1_berika
* V1_hjälpa_på_plats
* V1_kulturberika
* V1_återvandra

**Note:** DWTs "ordning och reda i flyktingpolitiken" and "självständig utrikespolitik" are too infrequent in the data to systematically analyse below. However, I must look into details of their frequency distributions. TBC ... 

### A terminological note:
Following Noble et al., 
* *genuine (naive) change* is defined as the angular distance of two vectors; 
* *rectified change* is the *t*-statistic of genuine change given the estimated noise distribution (i.e. the mean and standard deviation of the controls, given Bessel's correction); i.e. for a given word *w* and a temporal transition from *u_i* to *u_j* (e.g. from one year to the next): <br/>
	rectified change(*w*) = angular distance(*w*) - *M_c(w)* / *S_c(w)* * sqrt(1 + 1/*N_c*) <br/>
where *M_c* is the mean angular change of controls, *S_c* is the standard deviation of the angular change of the controls, and *N_c* is the number of controls. 

Noble et al. identify **4.781** as the critical value of rectified change corresponding to 99.95% confidence that a significant value has been identified. Accordingly,
* *significant change* can be defined as rectified change > 4.781.

~~**Question:** There is no reference for this value in Noble et al., but as fas as I can tell this is not the "classical" critical value of the *t*-distribution. Where does it come from? What am I missing?~~ (I have found the critical value 4.781)
### SGNS vs. BERT?
Comparing the performance of the models, three conclusions can be made:

#### 1. Considering genuine change, the SGNS and BERT models come to fundamentally different conclusions 
Table 2 shows Pearson's correlation of models, considering the genuine change of a GDWT at a *y_i* to *y_j* transition, where the value is not *NaN* (*n* = 154). It is clear that genuine change detected by the SGNS model is *negatively* correlated with the change detected by the BERT models. **Note: this needs further investigation. Is there a bug? We will return to this "disturbing" fact below.**

**TABLE 2: Pearson's *r* genuine change (n=154)**
|      |   SGNS |    NLI |    STS |    BIG |     KB |
|:--|--:|--:|--:|--:|--:|
| SGNS |  1     | -0.251 | -0.328 | -0.304 | -0.266 |
| NLI  | -0.251 |  1     |  0.959 |  0.967 |  0.946 |
| STS  | -0.328 |  0.959 |  1     |  0.99  |  0.946 |
| BIG  | -0.304 |  0.967 |  0.99  |  1     |  0.957 |
| KB   | -0.266 |  0.946 |  0.946 |  0.957 |  1     |

#### 2. BERT models are very similar
Also clear from Table 2 is that the four different sentence BERT models are strongly correlated. The strongest correlation is to be found for the two models trained through STS (*STS* and *BIG*), but also the model trained through the NLI task is strong related to those trained with the STS task. While KB's model is the one least correlated with the other BERT models, it is still strongly correlated with them, to such an extent that one might conclude that fine-tuning sentence-BERT on Flashback data (i.e. *NLI*, *STS*, *BIG*), has had quite limited effect on the results for detecting genuine change.

#### 3. Considering rectified change, the negative correlation between SGNS and BERT disapears
If we instead focus on rectified change, the SGNS model and the BERT models are in more agreement. Thus, one might conclude that looking at the statistical significance of change, rather than the "effect size", the models come to more similar conclusions. 

**TABLE 3: Pearson's *r* rectfied change (n=154)**
|      |   SGNS |   NLI |   STS |   BIG |    KB |
|:--|--:|--:|--:|--:|--:|
| SGNS |  1     | 0.485 | 0.552 | 0.501 | 0.381 |
| NLI  |  0.485 | 1     | 0.845 | 0.876 | 0.744 |
| STS  |  0.552 | 0.845 | 1     | 0.904 | 0.733 |
| BIG  |  0.501 | 0.876 | 0.904 | 1     | 0.799 |
| KB   |  0.381 | 0.744 | 0.733 | 0.799 | 1     |

Table 3 also shows that the BERT models are somewhat less strongly related. My guess is that this has to do with the randomness introduced when calculating rectified change. Using a higher value for the number of controls might make the difference less prominent. This observation should be pursued further, so there is not a bug in the code. 

### Detection of semantic change
Table 4 and 5 show the *significant* change (as defined above) detected by the SGNS and the STS BERT models. Note that some of the 0s in these models are really NaNs since every term is not present every year. The SGNS model suggests six cases of significant change: 
* N1_berikare from 2006 to 2007
* N1_globalist from 2007 to 2008
* N1_kulturberikare from 2006 to 2007 and again from 2009 to 2010
* N1_återvandring from 2017 to 2018 and again from 2018 to 2019

**TABLE 4: SGNS: Significant change; 1 for True, 0 for False**
|  |00:01 |01:02 |02:03 |03:04 |04:05 |05:06 |06:07 |07:08 |08:09 |09:10 |10:11 |11:12 |12:13 |13:14 |14:15 |15:16 |16:17 |17:18 |18:19 |19:20 |20:21 |21:22 |
|:--|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|
| A1_globalistisk | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| N1_berikare  | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| N1_förortsgäng  | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| N1_globalist | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| N1_kulturberikare  | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| N1_återvandring | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 |
| N2_återvandrare | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| V1_berika | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| V1_hjälpa_på_plats | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| V1_kulturberika | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| V1_återvandra| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |

The STS model is much more sensitive to "detect" significant change, as there is not less then 49 cases of change (only considering change from one year to the next where the frequency of both years is at least 10). For example, V1_berika is suggested to change almost every year.  

**TABLE 5: STS: Significant change; 1 for True, 0 for False (min. freq. = 10)**
|  |00:01 |01:02 |02:03 |03:04 |04:05 |05:06 |06:07 |07:08 |08:09 |09:10 |10:11 |11:12 |12:13 |13:14 |14:15 |15:16 |16:17 |17:18 |18:19 |19:20 |20:21 |21:22 |
|:--|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|
| A1_globalistisk | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 1 |
| N1_berikare  | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| N1_förortsgäng  | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| N1_globalist | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |
| N1_kulturberikare  | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| N1_återvandring | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 0 | 1 | 1 |
| N2_återvandrare | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| V1_berika | 1 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 1 | 1 | 1 | 1 |
| V1_hjälpa_på_plats | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 |
| V1_kulturberika | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| V1_återvandra| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 |

At this point, it is not perfectly clear what to conclude from this, but some points can be made. First, again, there is something suspect with the models, as they "behave" so differently. Second, the genuine change in the cases of significant change is often quite low. A change is often significant (as defined above), since the mean of the controls is even smaller and the standard deviation of the control samples are very small. For example, the semantic change of V1_berika from 2008 to 2009 is significant (rectified change = 5.84), with genuine change = 0.028 (mean of controls = 0.016, std=0.002). However, from the viewpoint of *similarity*, the vectors for V1_berika at 2008 and 2009 are *very* similar with a cosine similarity = 0.996. This seriously suggest a need to reconsider the notion of rectified change when considering BERT models. Table 6 shows the genuine change according to the BERT STS model. 

**TABLE 6: STS: Genuine change (min. freq. = 10)**
|  |00:01 |01:02 |02:03 |03:04 |04:05 |05:06 |06:07 |07:08 |08:09 |09:10 |10:11 |11:12 |12:13 |13:14 |14:15 |15:16 |16:17 |17:18 |18:19 |19:20 |20:21 |21:22 |
|:--|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|
| A1_globalistisk | nan  | nan  | nan  | nan  | nan  |0.249 |0.213 |0.143 |0.081 |0.088 |0.071 |0.053 |0.044 |0.048 |0.044 |0.051 |0.032 |0.037 |0.027 |0.032 |0.033 |0.045 |
| N1_berikare  |0.326 |0.375 |0.331 | nan  | nan  |0.216 |0.134 |0.044 |0.031 |0.031 |0.031 |0.037 |0.038 |0.05  |0.046 |0.055 |0.055 |0.055 |0.052 |0.061 |0.089 |0.104 |
| N1_förortsgäng  | nan  | nan  | nan  | nan  | nan  | nan  | nan  |0.317 |0.164 |0.2|0.244 |0.194 |0.191 |0.146 |0.113 |0.114 |0.153 |0.159 |0.128 |0.137 |0.147 |0.146 |
| N1_globalist | nan  | nan  | nan  | nan  |0.294 |0.128 |0.135 |0.129 |0.065 |0.055 |0.054 |0.064 |0.036 |0.036 |0.047 |0.039 |0.024 |0.027 |0.021 |0.028 |0.023 |0.045 |
| N1_kulturberikare  |0.154 |0.175 |0.241 |0.224 |0.117 |0.111 |0.085 |0.038 |0.028 |0.029 |0.026 |0.029 |0.035 |0.037 |0.042 |0.052 |0.057 |0.049 |0.045 |0.06  |0.056 |0.06  |
| N1_återvandring | nan  | nan  | nan  | nan  | nan  | nan  |0.164 |0.106 |0.068 |0.075 |0.08  |0.062 |0.054 |0.049 |0.054 |0.062 |0.051 |0.055 |0.03  |0.021 |0.029 |0.022 |
| N2_återvandrare | nan  | nan  | nan  | nan  | nan  | nan  |0.3|0.182 |0.353 |0.309 |0.322 |0.381 |0.24  |0.255 |0.276 |0.254 |0.243 |0.209 |0.125 |0.193 |0.259 |0.301 |
| V1_berika |0.193 |0.195 |0.186 |0.156 |0.082 |0.071 |0.082 |0.03  |0.028 |0.026 |0.021 |0.024 |0.026 |0.029 |0.024 |0.026 |0.04  |0.027 |0.032 |0.038 |0.034 |0.046 |
| V1_hjälpa_på_plats | nan  | nan  | nan  | nan  | nan  | nan  | nan  |0.193 |0.154 |0.137 |0.094 |0.069 |0.069 |0.072 |0.042 |0.048 |0.058 |0.059 |0.076 |0.065 |0.072 |0.1|
| V1_kulturberika |0.237 |0.273 | nan  | nan  |0.275 |0.195 |0.121 |0.048 |0.038 |0.042 |0.041 |0.039 |0.047 |0.056 |0.059 |0.051 |0.058 |0.076 |0.078 |0.083 |0.086 |0.094 |
| V1_återvandra| nan  | nan  | nan  | nan  |0.349 |0.277 |0.226 |0.119 |0.108 |0.107 |0.079 |0.096 |0.099 |0.079 |0.099 |0.142 |0.122 |0.096 |0.054 |0.054 |0.048 |0.05  |

### Changing spread?
For the BERT models, the spread of meaning of a word *w* at year *u* is defined as the mean of pairwise similarity of *w*'s vectors at *u*. At *u_i* we can calculate the mean and standard deviation of the spread of *w* before *u_i*. Table 7 shows how many standard deviations that *w* changes at *u* given the spread of previous years. It seems that the "internal" similarity of DWT usages each year decreases over time. 

**TABLE 7: STS: Change of spread in std of spread upto the year *u* (min. freq. = 10)**
|  |  02 |  03 |  04 |  05 |  06 |  07 |  08 |  09 | 10 |  11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19 | 20 |  21 | 22 |
|:--|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|
| A1_globalistisk | nan | nan | nan | nan | nan |0.1  |  -0.02 |0.3  |  0.07 |  -0.05 |  0.02 | -0.13 | -0.38 | -0.37 | -0.56 | -0.42 | -0.75 | -0.74 | -0.73 |  -0.84 | -0.56 |
| N1_berikare  | nan |  -0.34 | nan |  -0.92 |  -1.44 |  -0.8  |  -0.66 |  -0.64 | -0.64 |  -0.53 | -0.45 | -0.39 | -0.33 | -0.4  | -0.45 | -0.57 | -0.3  | -0.43 | -0.3  |  -0.51 | -0.17 |
| N1_förortsgäng  | nan | nan | nan | nan | nan | nan |  -0.23 |  -0.66 | -0.64 |  -1.03 | -0.28 | -0.57 | -0.21 | -0.48 | -0.77 | -1.09 | -0.55 | -0.1  | -0.45 |  -0.04 | -1.01 |
| N1_globalist | nan | nan | nan | nan |  -0.58 |  -0.94 |  -0.28 |  -0.35 | -0.57 |  -0.69 |  0.28 |  0.1  | -0.23 | -0.57 | -0.36 | -0.53 | -0.55 | -0.51 | -0.46 |  -0.46 | -0.45 |
| N1_kulturberikare  |0.22 | **-16.03** |1.9  |  -0.69 |1.76 |  -0.16 |  -0.05 |  -0.08 |  0.14 |0.19 |  0.34 |  0.33 |  0.27 |  0.24 | -0.47 | -0.29 |  0.57 | -0.05 |  0.38 |  -0.12 | -0.35 |
| N1_återvandring | nan | nan | nan | nan | nan | nan |0.28 |  -1.09 | -0.77 |0.29 |  0.4  | -0.58 | -0.44 | -0.3  |  0.08 | -1.75 | **-2.47** | **-2.17** | -1.45 |  -0.98 | -1.11 |
| N2_återvandrare | nan | nan | nan | nan | nan | nan |  -0.8  | nan | -0.87 | nan | -0.58 | -0.59 |  0.23 | -1.17 | -0.38 | -0.45 | -0.37 | -0.71 | -0.29 | nan |  0.2  |
| V1_berika |1.72 |**5.92** |0.67 |  -0.92 |  -0.35 |0.61 |0.58 |0.19 |  0.55 |0.11 |  0.72 |  0.58 | -0.4  | -0.14 | -0.55 | -0.08 |  0.11 | -0.02 | -0.16 |  -0.44 | -0.32 |
| V1_hjälpa_på_plats | nan | nan | nan | nan | nan | nan | nan |**2.07** |  0.04 |  -0.85 | -0.45 | -0.13 | -1.21 | -0.99 | -1.27 | -1.15 | -0.8  | -1.29 | -0.19 |  -1.17 | -0.26 |
| V1_kulturberika |0.58 | nan |  **-2.54** |1.19 |  -0.36 |  -0.45 |  -0.38 |  -0.15 | -0.38 |  -0.01 | -0.04 | -0.29 | -0.56 | -0.3  |  0.18 | -0.33 | -0.46 | -0.09 |  0.99 |1.04 |  0.07 |
| V1_återvandra| nan | nan | nan | nan | nan |0.08 |0.07 |0.56 |  0.02 |0.27 |  0.26 |  0.99 |  0.02 | -0.12 | -0.71 |  0.22 |  0.1  | -0.04 |  0.48 |  -0.06 |  0.09 |

The largest negative changes of spread are found for N1_kulturberikare in 2003 and V1_kulturberika in 2004. 

### Law of confirmity?
Hamilton et al. propose a *law of conformity* of semantic change saying that frequently used words change less than less frequent words. This tendency can here be be confirmed for the BERT models.
Given all GDWT with a min. freq. of 10, there is a negative correlation between the genuine change of a word from *u_i* to *u_i+1* and the log frequency at *u_i* (from-year) or *u_i+1* (to-year). 

**TABLE 8: BERT models, Pearson's corr. (min. freq. 10); all correlations are sign. at 0.0001 level**
| Model | corr, from-year | corr,  to-year |
|-------|----------------:|----------------|
| NLI   | -0.370          | -0.374         |
| STS   | -0.512          | -0.498         |
| BIG   | -0.488          | -0.473         |
| KB    | -0.449          | -0.473         |

~~Above, in the sect. "SGNS vs. BERT", we concluded that the while the BERT models are strongly correlated to each other, they bare negatively correlated with the SGSN model. This mismatch re-appear here, as the SGNS model show the *opposite* trend of the BERT models. Since the law of confirmity has been observed elsewhere, this suggests that something is wrong with the SGNS model (rather than the BERT models). Considering *all* words, the relationship between genuine change and log frequency is positive: considering from-years, *r* = 0.394, *p* < 0.0001 (*N* = 3342680); considering to-years, *r* = 0.475, *p* < 0.0001 (*N* = 3342680). Focusing more narrowly on GDWTs, the positive relationship consists; *r* = 0.384 (from-year); *r* = 0.545 (to_year); both sign. far beyond p = 0.0001. Since these values basically are reversed of those of the BERT models, one might suspect that "change" has been confused with "similarity" somewhere in the code, but I have not yet found such an error. *The search continues ...* ~~ (I have been able to reproduce the negative correlation found in Hamiltion et al. by, as they did, only consider the 10 000 most common words.)
