{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remaining problems:\n",
    "* index of target word in sentences, where punctuation is not separated (tokenized), e.g. \"jag tycker att relationen till min sambos ursprung berikar mig enormt!\"\n",
    "* compounds, e.g., \"globalistelit\"; tokeniser does not necesserily identigy boundery for `globalist|elit`; if it does, this would solve it\n",
    "* more than one target in sentence (consider `sent.split(\" \").index(word)`)\n",
    "    * use `re` for strings somehow??\n",
    "* what is the difference between `T5TokenizerFast` and `T5Tokenizer` other than the latter lacks a method for `word_ids()`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from difflib import SequenceMatcher\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "# from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/transformers/v4.1.1/main_classes/tokenizer.html#transformers.PreTrainedTokenizerFast\n",
    "from transformers import T5TokenizerFast, T5EncoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T5tokenizer = T5TokenizerFast.from_pretrained(\"t5-small\")\n",
    "# T5model = T5EncoderModel.from_pretrained(\"t5-small\")\n",
    "# T5tokenizer = T5TokenizerFast.from_pretrained(\"t5-large\", model_max_length=512)\n",
    "# T5model = T5EncoderModel.from_pretrained(\"t5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = Counter()\n",
    "# path = Path(\"../files\")\n",
    "# i = 0\n",
    "# for file in os.listdir(path):\n",
    "#     with open(path / file, encoding=\"utf-8\") as f:\n",
    "#         for line in f:\n",
    "#             line = line.split(\"\\t\")\n",
    "#             counter.update(line[0].split(\"; \"))\n",
    "#             print(i, end=\"\\r\")\n",
    "#             i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted([k for k in counter.keys() if not k.startswith(\"X_hjälpa\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted([k for k in counter.keys() if k.startswith(\"X_hjälpa\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore = tuple([\"P1_\", \"V1_hjälpa\", \"V2_hjälpa\", \"X_hjälpa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted([k for k in counter.keys() if not k.startswith(ignore)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_split(string):\n",
    "    k, v = (f\"{string.split(' -> ')[1]}_{string.split()[-1]}\", string.split(' -> ')[2])\n",
    "    return k,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'N1_förortsgäng': '\\\\bförortsgäng(|et|en|s|ets|ens)\\\\b', 'N1C_förortsgängX': '\\\\b(förortsgäng)s?((?!\\\\b|et|en|s\\\\b).*?)\\\\b', 'N1_återvandring': '\\\\båtervandring(|en|ar|s|ens|ars|arna|arnas)\\\\b', 'N2_återvandrare': '\\\\båtervandrar(e|en|es|ens|na|nas)\\\\b', 'N1C_återvandringsX': '\\\\b(återvandring)s?((?!\\\\b|en|ar|s\\\\b).*?)\\\\b', 'N2C_återvandrarX': '\\\\b(återvandrar)((?!\\\\b|e\\\\b|en|es|na).*?)\\\\b', 'V1_återvandra': '\\\\båtervandra(\\\\b|r|d|de|t|nde|ndet)\\\\b', 'N1_berikare': '\\\\bberikar(e|en|na|es|ens|nas)\\\\b', 'N1C_berikareX': '\\\\b(berikar)((?!\\\\b|e\\\\b|en|na|es).*?)\\\\b', 'V1_berika': '\\\\bberik(a|ar|ad|ade|at|ande|as|ats|ades)\\\\b', 'N1_kulturberikare': '\\\\bkulturberikar(e|en|na|es|ens|nas)\\\\b', 'N1C_kulturberikarX': '\\\\b(kulturberikar)((?!\\\\b|e\\\\b|en|na|es).*?)\\\\b', 'V1_kulturberika': '\\\\bkulturberik(a|ar|ad|ade|at|ande|as|ats|ades)\\\\b', 'P1_ordning_och_reda_i_flyktingpolitiken': 'ordning och reda i flyktingpolitiken', 'N1_globalist': '\\\\bglobalist(|en|er|erna|s|ens|ers|ernas)\\\\b', 'N1C_globalistX': '\\\\b(globalist)((?!\\\\b|en|er|s\\\\b|isk).*?)\\\\b', 'A1_globalistisk': '\\\\bglobalistisk(|a|t|e)\\\\b', 'V1_hjälpa_på_plats': '\\\\bhjälp(a|er|te|t|as|ande)\\\\b på plats\\\\b', 'V2_hjälpa_X_på_plats': '\\\\bhjälp(a|er|te|t|as|ande)\\\\b .*? på plats\\\\b', 'P1_självständig_utrikespolitik': '\\\\bsjälvständiga? utrikespolitik(|en|s|ens|er|ers|ernas)\\\\b'}\n"
     ]
    }
   ],
   "source": [
    "#with open(\"/home/max/Documents/mlt/thesis/dw_change/data/utils/dwts.paradigm\") as f:\n",
    "with open(\"dwts.paradigm.txt\", encoding=\"utf-8\") as f:\n",
    "    paradigms = [line.strip(\"\\n\") for line in f.readlines() if not line.startswith(\"#\")]\n",
    "paradigms = [p for p in paradigms if p != \"\"]\n",
    "paradigms = [p.split(\" #\")[0] for p in paradigms]\n",
    "paradigms = dict([p_split(paradigm) for paradigm in paradigms])\n",
    "print(paradigms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_for_idx(sentence):\n",
    "    encoded = tokenizer.encode_plus(sentence, return_tensors=\"pt\")\n",
    "    inbetween = tokenizer.decode(encoded[\"input_ids\"][0], skip_special_tokens = True, clean_up_tokenization_spaces=True)\n",
    "    return inbetween"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(sentence, exact_match, lemma, model, device = \"cpu\", only_check = True):\n",
    "    if lemma.startswith(\"X\"): # X_globalist\n",
    "        true_lemma = lemma.split(\"_\")[-1] \n",
    "        true_wf = true_lemma + exact_match.split(true_lemma)[-1]\n",
    "    else: \n",
    "        if lemma.endswith(\"X\"): # N1C_globalistX\n",
    "            true_lemma = lemma.split(\"_\")[1][:-1]\n",
    "            true_wf = lemma.split(\"_\")[1][:-1] #true_lemma\n",
    "        else: # N1_globalist\n",
    "            true_lemma = lemma.split(\"_\")[1]\n",
    "            true_wf = exact_match\n",
    "\n",
    "    # sentence=loop_for_idx(sentence)\n",
    "    encoded = tokenizer.encode_plus(sentence, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    #print(encoded)\n",
    "    tokens = encoded.tokens() #[tokenizer.decode(wid) for wid in encoded[\"input_ids\"][0]]\n",
    "\n",
    "    # short_tokens = \"\".join([tok.replace(\"▁\", \" \") for tok in tokens]).replace(\"</s>\", \"\")\n",
    "\n",
    "    # if exact_match not in short_tokens.split():\n",
    "    #     print()\n",
    "    #     print(sentence.replace(exact_match, exact_match.upper()))\n",
    "    #     #print(short_tokens)\n",
    "    #     encoded = tokenizer.encode_plus(sentence, return_tensors=\"pt\", truncation=\"only_second\", max_length=512)\n",
    "    #     tokens = encoded.tokens() #[tokenizer.decode(wid) for wid in encoded[\"input_ids\"][0]]\n",
    "    #     sentence = \"\".join([tok.replace(\"▁\", \" \") for tok in tokens]).replace(\"</s>\", \"\")\n",
    "\n",
    "    try:\n",
    "        idx = sentence.split().index(exact_match) # will not match tokenizer; hence `map_tok()`\n",
    "        # inbetween = tokenizer.decode(encoded[\"input_ids\"][0], skip_special_tokens = True, clean_up_tokenization_spaces=True)\n",
    "        \n",
    "        #print(inbetween)\n",
    "        # idx = inbetween.split().index(exact_match)\n",
    "    except:\n",
    "        print(\"Oops! `exact_match` not in sentence. |\", lemma, \"|\", exact_match, \"|\", sentence)\n",
    "        return\n",
    "    \n",
    "    token_ids_word = np.where(np.array(encoded.word_ids()) == idx)[0]\n",
    "    if token_ids_word.tolist() == []:\n",
    "        return\n",
    "    #print(token_ids_word)\n",
    "    \n",
    "    if lemma.endswith(\"X\") or lemma.startswith(\"X\"):\n",
    "        start_with = min(token_ids_word)\n",
    "        outer = start_with\n",
    "        top    = 0\n",
    "\n",
    "        if lemma.startswith(\"X\"):\n",
    "            for i, idx in enumerate(token_ids_word):\n",
    "                if true_lemma.startswith(tokens[idx].replace(\"▁\", \"\")):\n",
    "                    token_ids_word = token_ids_word[i:]\n",
    "                    start_with = min(token_ids_word)\n",
    "                    break\n",
    "\n",
    "        for i in token_ids_word: \n",
    "            i = i + 1\n",
    "            candidate = \"\".join([tok.replace(\"▁\", \"\") for tok in tokens[min(token_ids_word):i]])\n",
    "            score = SequenceMatcher(None, true_wf, candidate).ratio()\n",
    "            if score >= top:\n",
    "                top = score\n",
    "                outer = i\n",
    "\n",
    "        token_ids_word = np.arange(start_with, outer) # arrange\n",
    "        #print(token_ids_word)\n",
    "\n",
    "    if only_check:\n",
    "        #print(\" \".join(tokens))#, end = \"\\r\")\n",
    "        try:\n",
    "            tokens = tokens[token_ids_word[0]:token_ids_word[-1]+1]\n",
    "        except:\n",
    "            print(\"\\nERROR: No `token_ids_word` |\", lemma, token_ids_word, \"|\", \" \".join(encoded.tokens()))\n",
    "            print(sentence)\n",
    "            return\n",
    "        tokens_short = \"\".join([tok.replace(\"_\", \"\") for tok in tokens])\n",
    "        if not \"hjälpa_på\" in lemma:\n",
    "            if tokens_short != true_wf:\n",
    "                if sum([dwe in tokens_short for dwe in [\"globali\", \"berika\", \"återvandr\", \"förortsgäng\"]]) == 0: # note short form for globalist (globali) due to t5 tokenization\n",
    "                    print(f\"{lemma} | {true_lemma} | {exact_match} | {true_wf}  >>> {' '.join(tokens)} <<<  ({token_ids_word}):\\n1:{sentence})\\n2:{' '.join(encoded.tokens())}\")\n",
    "        return\n",
    "\n",
    "    encoded.to(device)\n",
    "    # with torch.no_grad():\n",
    "    #     output = model(**encoded)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.encoder(\n",
    "            input_ids=encoded[\"input_ids\"], \n",
    "            attention_mask=encoded[\"attention_mask\"], \n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "    last_hidden = output.last_hidden_state.squeeze()\n",
    "    word_tokens_output = last_hidden[token_ids_word]\n",
    "\n",
    "    return word_tokens_output.mean(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tok(sentence):\n",
    "    # sentence = re.sub(r\" ([\\.,!?])\", r\"\\1\", sentence)\n",
    "    sentence = sentence.replace(\"-\", \" - \")\n",
    "    sentence = sentence.replace(\".\", \" . \")\n",
    "    sentence = sentence.replace(\"+\", \" + \")\n",
    "    sentence = sentence.replace(\"&\", \" & \")\n",
    "    sentence = sentence.replace(\"@\", \" @ \")\n",
    "    sentence = sentence.replace(\":\", \" : \")\n",
    "    sentence = sentence.replace(\"*\", \" * \")\n",
    "    sentence = sentence.replace(\"^\", \" ^ \")\n",
    "    sentence = sentence.replace(\"ü\", \"u\")\n",
    "    sentence = sentence.replace(\"$\", \"s\")\n",
    "    # sentence = sentence.replace(\">\", \"\")\n",
    "    # sentence = re.sub(r\"([a-zåäö]):([a-zåäö])\", r\"\\1 : \\2\", sentence)\n",
    "    sentence = sentence.replace(\"=) \", \"\")\n",
    "    sentence = sentence.replace(\">= \", \"\")\n",
    "    sentence = sentence.replace(\"=>\", \"\")\n",
    "    sentence = sentence.replace(\">>\", ' \" ')\n",
    "    sentence = sentence.replace(\"<<\", ' \" ')\n",
    "    sentence = sentence.replace(\"''\", ' \" ')\n",
    "    sentence = sentence.replace(\"´\", \" ' \") # see below\n",
    "    sentence = sentence.replace(\"(\", \" ( \") # see below\n",
    "    sentence = sentence.replace(\")\", \" ) \") # see below\n",
    "    sentence = sentence.replace(\"| \", \"\")\n",
    "    sentence = sentence.replace(\"överraskningsberika\", \"överrasknings berika\")\n",
    "    sentence = sentence.replace(\"\\U0001F923\", \"\")\n",
    "    sentence = re.sub(r\"([a-zåäö])(['`])([a-zåäö])\", r\"\\1 \\2 \\3\", sentence)\n",
    "    sentence = re.sub(r\"([()\\?=%!<>~«])([a-zåäö0-9])\", r\"\\1 \\2\", sentence)\n",
    "    sentence = re.sub(r\"([a-zåäö0-9])([()\\?=%!<>~«])\", r\"\\1 \\2\", sentence)\n",
    "    sentence = re.sub(r\"([0-9]),([0-9])\", r\"\\1 , \\2\", sentence)\n",
    "    sentence = re.sub(r\"#+\", \" * \", sentence)\n",
    "    sentence = re.sub(r\"([=¤])+\", r\"\\1\", sentence)\n",
    "    sentence = re.sub(r\" +\", \" \", sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(\n",
    "    model,\n",
    "    directory,\n",
    "    vector_dir,\n",
    "    paradigms,\n",
    "    ignore,\n",
    "    device=\"cpu\",\n",
    "    only_check = True,\n",
    "    dev_restriction=None,\n",
    "    re_start = None\n",
    "):\n",
    "\n",
    "    directory = Path(directory)\n",
    "    vector_dir = Path(vector_dir)\n",
    "\n",
    "    isExist = os.path.exists(vector_dir)\n",
    "    if not isExist:\n",
    "        os.makedirs(vector_dir)  \n",
    "    \n",
    "    files = os.listdir(directory)\n",
    "    if re_start != None:\n",
    "        years = sorted([int(y.replace(\".txt\", \"\")) for y in files])\n",
    "        files = [f\"{y}.txt\" for y in years if y >= re_start]\n",
    "    \n",
    "    if dev_restriction != None:\n",
    "        files = dev_restriction\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for file in files:\n",
    "        print()\n",
    "        print(file)\n",
    "        with open(directory / file, encoding=\"utf-8\") as f, open(vector_dir / file, \"w\", encoding=\"utf-8\") as out:\n",
    "            for i, line in enumerate(f):\n",
    "                if i % 10 == 0:\n",
    "                    print(i, end=\"\\r\")\n",
    "                lemma, n, sentence = tuple(line.strip(\"\\n\").split(\"\\t\"))\n",
    "\n",
    "                sentence = map_tok(sentence)\n",
    "\n",
    "                if int(n) == 1: \n",
    "                    if lemma.startswith(ignore):\n",
    "                        continue\n",
    "                    if lemma in paradigms:\n",
    "                        regex = paradigms[lemma]\n",
    "                        regex = re.compile(regex)    \n",
    "                        exact_match = re.search(regex, sentence)\n",
    "                        if exact_match == None:\n",
    "                            print(\"ERROR:\", lemma, \"||\", sentence)\n",
    "                        exact_match = exact_match.group()                        \n",
    "                    else:\n",
    "                        regex = re.compile(f\"\\\\b[0-9a-zåäö]*{lemma.split('_')[-1]}.*?\\\\b\")\n",
    "                        exact_match = re.search(regex, sentence)\n",
    "                        if exact_match == None:\n",
    "                            print(\"ERROR:\", lemma, \"|\", regex, \"|\", sentence)\n",
    "                        exact_match = exact_match.group()\n",
    "                    vector = get_word_vector(sentence, exact_match, lemma, model, device, only_check)\n",
    "                    if only_check or vector == None:\n",
    "                        continue\n",
    "                        \n",
    "                    vector = \" \".join([str(v) for v in vector.tolist()]) # consider torch.save(tensor, 'file.pt')\n",
    "                    out.write(f\"{lemma}\\t{vector}\\n\")                   \n",
    "\n",
    "                else: # two instances of the same lemma = problem\n",
    "                    for l in lemma.split(\"; \"):\n",
    "                        if l.startswith(ignore):\n",
    "                            continue                        \n",
    "                        if l in paradigms:\n",
    "                            regex = paradigms[l]\n",
    "                            regex = re.compile(regex)    \n",
    "                            exact_match = re.search(regex, sentence).group() \n",
    "                        else:\n",
    "                            regex = re.compile(f\"\\\\b[0-9a-zåäö]*{l.split('_')[-1]}.*?\\\\b\")\n",
    "                            exact_match = re.search(regex, sentence).group()\n",
    "                        vector = get_word_vector(sentence, exact_match, l, model, device, only_check)\n",
    "                        if only_check or vector == None:\n",
    "                            continue\n",
    "                        vector = \" \".join([str(v) for v in vector.tolist()])\n",
    "                        out.write(f\"{l}\\t{vector}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google-t5/t5-3b\" # 't5-small' # \"google-t5/t5-3b\"\n",
    "# model_name = \"google-t5/t5-small\"\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_name, model_max_length=512)\n",
    "model = T5EncoderModel.from_pretrained(model_name)\n",
    "short_name = model_name.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2000.txt\n",
      "120\n",
      "2001.txt\n",
      "60\n",
      "2002.txt\n",
      "40\n",
      "2003.txt\n",
      "30\n",
      "2004.txt\n",
      "120\n",
      "2005.txt\n",
      "180\n",
      "2006.txt\n",
      "520\n",
      "2007.txt\n",
      "1580\n",
      "2008.txt\n",
      "3780\n",
      "2009.txt\n",
      "4120\n",
      "2010.txt\n",
      "5060\n",
      "2011.txt\n",
      "4360\n",
      "2012.txt\n",
      "4600\n",
      "2013.txt\n",
      "4140\n",
      "2014.txt\n",
      "3800\n",
      "2015.txt\n",
      "3730\n",
      "2016.txt\n",
      "4840\n",
      "2017.txt\n",
      "5130\n",
      "2018.txt\n",
      "11570\n",
      "2019.txt\n",
      "8640\n",
      "2020.txt\n",
      "7530\n",
      "2021.txt\n",
      "7120\n",
      "2022.txt\n",
      "Oops! `exact_match` not in sentence. | N1C_globalistX | globalistetna | storbritanien o indien har idag kanske inte sa mycket att gora med varandra , dock sa hoppas sa klart imperialisterna（globalistetna）att ha indien pa sin sida\n",
      "CPU times: total: 1h 38min 37s\n",
      "Wall time: 1h 31min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "get_word_embeddings(\n",
    "    model=model, \n",
    "    directory=Path(\"../data/corpus/fb_pol_files/\"), #\"C:\\Users\\xbohma\\Desktop\\work\\work\\data\\corpus\\files\"\n",
    "    vector_dir=f\"../data/vectors/fb_pol/{short_name}\", #\"../bert-base-swedish-cased/\", # \"C:\\Users\\xbohma\\Desktop\\work\\work\\data\\vectors\\bert-base-swedish-cased\"\n",
    "    paradigms=paradigms,\n",
    "    ignore=ignore,\n",
    "    device=\"cuda\",\n",
    "    only_check = False,\n",
    "    #dev_restriction = [\"2018.txt\"]\n",
    "    #re_start = 2022\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# get_word_embeddings(\n",
    "#     model=model, \n",
    "#     directory=Path(\"../data/corpus/files\"), #\"C:\\Users\\xbohma\\Desktop\\work\\work\\data\\corpus\\files\"\n",
    "#     vector_dir=f\"../data/vectors/{short_name}\", #\"../bert-base-swedish-cased/\", # \"C:\\Users\\xbohma\\Desktop\\work\\work\\data\\vectors\\bert-base-swedish-cased\"\n",
    "#     paradigms=paradigms,\n",
    "#     ignore=ignore,\n",
    "#     device=\"cpu\",\n",
    "#     only_check = True,\n",
    "#     #dev_restriction = [\"2018.txt\"]\n",
    "#     #re_start = 2022\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
