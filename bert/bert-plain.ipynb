{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remaining problems:\n",
    "* index of target word in sentences, where punctuation is not separated (tokenized), e.g. \"jag tycker att relationen till min sambos ursprung berikar mig enormt!\"\n",
    "* compounds, e.g., \"globalistelit\"; tokeniser does not necesserily identigy boundery for `globalist|elit`; if it does, this would solve it\n",
    "* more than one target in sentence (consider `sent.split(\" \").index(word)`)\n",
    "    * use `re` for strings somehow??\n",
    "* what is the difference between `T5TokenizerFast` and `T5Tokenizer` other than the latter lacks a method for `word_ids()`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from difflib import SequenceMatcher\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = Counter()\n",
    "# path = Path(\"../files\")\n",
    "# i = 0\n",
    "# for file in os.listdir(path):\n",
    "#     with open(path / file, encoding=\"utf-8\") as f:\n",
    "#         for line in f:\n",
    "#             line = line.split(\"\\t\")\n",
    "#             counter.update(line[0].split(\"; \"))\n",
    "#             print(i, end=\"\\r\")\n",
    "#             i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted([k for k in counter.keys() if not k.startswith(\"X_hjälpa\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted([k for k in counter.keys() if k.startswith(\"X_hjälpa\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore = tuple([\"P1_\", \"V1_hjälpa\", \"V2_hjälpa\", \"X_hjälpa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted([k for k in counter.keys() if not k.startswith(ignore)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_split(string):\n",
    "    k, v = (f\"{string.split(' -> ')[1]}_{string.split()[-1]}\", string.split(' -> ')[2])\n",
    "    return k,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#with open(\"/home/max/Documents/mlt/thesis/dw_change/data/utils/dwts.paradigm\") as f:\n",
    "with open(\"dwts.paradigm.txt\", encoding=\"utf-8\") as f:\n",
    "    paradigms = [line.strip(\"\\n\") for line in f.readlines() if not line.startswith(\"#\")]\n",
    "paradigms = [p for p in paradigms if p != \"\"]\n",
    "paradigms = [p.split(\" #\")[0] for p in paradigms]\n",
    "paradigms = dict([p_split(paradigm) for paradigm in paradigms])\n",
    "print(paradigms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loop_for_idx(sentence):\n",
    "#     encoded = tokenizer.encode_plus(sentence, return_tensors=\"pt\")\n",
    "#     inbetween = tokenizer.decode(encoded[\"input_ids\"][0], skip_special_tokens = True, clean_up_tokenization_spaces=True)\n",
    "#     return inbetween"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(sentence, exact_match, lemma, model, device = \"cpu\", only_check = True):\n",
    "    if lemma.startswith(\"X\"): # X_globalist\n",
    "        true_lemma = lemma.split(\"_\")[-1] \n",
    "        true_wf = true_lemma + exact_match.split(true_lemma)[-1]\n",
    "    else: \n",
    "        if lemma.endswith(\"X\"): # N1C_globalistX\n",
    "            true_lemma = lemma.split(\"_\")[1][:-1]\n",
    "            true_wf = lemma.split(\"_\")[1][:-1] #true_lemma\n",
    "        else: # N1_globalist\n",
    "            true_lemma = lemma.split(\"_\")[1]\n",
    "            true_wf = exact_match\n",
    "\n",
    "    # sentence=loop_for_idx(sentence)\n",
    "    encoded = tokenizer.encode_plus(sentence, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    #print(encoded)\n",
    "    tokens = [tokenizer.decode(wid) for wid in encoded[\"input_ids\"][0]]\n",
    "\n",
    "    try:\n",
    "        idx = sentence.split().index(exact_match) # will not match tokenizer; hence `map_tok()`\n",
    "        # inbetween = tokenizer.decode(encoded[\"input_ids\"][0], skip_special_tokens = True, clean_up_tokenization_spaces=True)\n",
    "        \n",
    "        #print(inbetween)\n",
    "        # idx = inbetween.split().index(exact_match)\n",
    "    except:\n",
    "        #print(\"Oops! `exact_match` not in sentence.\", lemma, exact_match, sentence)\n",
    "        return\n",
    "    \n",
    "    token_ids_word = np.where(np.array(encoded.word_ids()) == idx)[0]\n",
    "    #print(token_ids_word)\n",
    "    \n",
    "    if lemma.endswith(\"X\") or lemma.startswith(\"X\"):\n",
    "        start_with = min(token_ids_word)\n",
    "        outer = start_with\n",
    "        top    = 0\n",
    "\n",
    "        if lemma.startswith(\"X\"):\n",
    "            for i, idx in enumerate(token_ids_word):\n",
    "                if true_lemma.startswith(tokens[idx].replace(\"##\", \"\")):\n",
    "                    token_ids_word = token_ids_word[i:]\n",
    "                    start_with = min(token_ids_word)\n",
    "                    break\n",
    "\n",
    "        for i in token_ids_word: \n",
    "            i = i + 1\n",
    "            candidate = \"\".join([tok.replace(\"##\", \"\") for tok in tokens[min(token_ids_word):i]])\n",
    "            score = SequenceMatcher(None, true_wf, candidate).ratio()\n",
    "            if score >= top:\n",
    "                top = score\n",
    "                outer = i\n",
    "\n",
    "        token_ids_word = np.arange(start_with, outer) # arrange\n",
    "        #print(token_ids_word)\n",
    "\n",
    "    if only_check:\n",
    "        #print(\" \".join(tokens))#, end = \"\\r\")\n",
    "        tokens = tokens[token_ids_word[0]:token_ids_word[-1]+1]\n",
    "        tokens_short = \"\".join([tok.replace(\"##\", \"\") for tok in tokens])\n",
    "        # if ' '.join(tokens) not in [\"global ##ista\",\n",
    "        #                             \"global ##istor\",\n",
    "        #                             \"global ##istik\", \n",
    "        #                             \"global ##istr\", \n",
    "        #                             \"ber ##ikar\", \n",
    "        #                             \"ober ##ikat\",\n",
    "        #                             \"avb ##erik ##a\",\n",
    "        #                             \"återv ##and ##ring\"\n",
    "        #                            ]: # since those are OK\n",
    "        if not \"hjälpa_på\" in lemma:\n",
    "            if tokens_short != true_wf:\n",
    "                if sum([dwe in tokens_short for dwe in [\"globalist\", \"berika\", \"återvandr\", \"förortsgäng\"]]) == 0:\n",
    "                    print(f\"{lemma} | {true_lemma} | {exact_match} | {true_wf}  >>> {' '.join(tokens)} <<<  ({token_ids_word}):\\n1:{sentence}\")#\\n2:{inbetween}\")\n",
    "        return\n",
    "\n",
    "    encoded.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded)\n",
    "        # try:\n",
    "        #     output = model(**encoded)\n",
    "        # except:\n",
    "        #     print(encoded[\"input_ids\"].shape)\n",
    "\n",
    "    last_hidden = output.last_hidden_state.squeeze()\n",
    "    word_tokens_output = last_hidden[token_ids_word]\n",
    "\n",
    "    return word_tokens_output.mean(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tok(sentence):\n",
    "    # sentence = re.sub(r\" ([\\.,!?])\", r\"\\1\", sentence)\n",
    "    sentence = sentence.replace(\"-\", \" - \")\n",
    "    sentence = sentence.replace(\".\", \" . \")\n",
    "    sentence = sentence.replace(\"+\", \" + \")\n",
    "    sentence = sentence.replace(\"&\", \" & \")\n",
    "    sentence = sentence.replace(\":\", \" : \")\n",
    "    sentence = sentence.replace(\"*\", \" * \")\n",
    "    sentence = sentence.replace(\"^\", \" ^ \")\n",
    "    sentence = sentence.replace(\"ü\", \"u\")\n",
    "    sentence = sentence.replace(\"$\", \"s\")\n",
    "    # sentence = sentence.replace(\">\", \"\")\n",
    "    # sentence = re.sub(r\"([a-zåäö]):([a-zåäö])\", r\"\\1 : \\2\", sentence)\n",
    "    sentence = sentence.replace(\"=) \", \"\")\n",
    "    sentence = sentence.replace(\">= \", \"\")\n",
    "    sentence = sentence.replace(\"=>\", \"\")\n",
    "    sentence = sentence.replace(\">>\", ' \" ')\n",
    "    sentence = sentence.replace(\"<<\", ' \" ')\n",
    "    sentence = sentence.replace(\"| \", \"\")\n",
    "    sentence = re.sub(r\"([a-zåäö])(['`])([a-zåäö])\", r\"\\1 \\2 \\3\", sentence)\n",
    "    sentence = re.sub(r\"([)\\?=%!<>~«])([a-zåäö0-9])\", r\"\\1 \\2\", sentence)\n",
    "    sentence = re.sub(r\"([a-zåäö0-9])([\\))\\?=%!<>~«])\", r\"\\1 \\2\", sentence)\n",
    "    sentence = re.sub(r\"([0-9]),([0-9])\", r\"\\1 , \\2\", sentence)\n",
    "    sentence = re.sub(r\"#+\", \" * \", sentence)\n",
    "    sentence = re.sub(r\"([=¤])+\", r\"\\1\", sentence)\n",
    "    sentence = re.sub(r\" +\", \" \", sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(\n",
    "    model,\n",
    "    directory,\n",
    "    vector_dir,\n",
    "    paradigms,\n",
    "    ignore,\n",
    "    device=\"cpu\",\n",
    "    only_check = True,\n",
    "    re_start = None\n",
    "):\n",
    "\n",
    "    directory = Path(directory)\n",
    "    vector_dir = Path(vector_dir)\n",
    "    files = os.listdir(directory)\n",
    "    if re_start != None:\n",
    "        years = sorted([int(y.replace(\".txt\", \"\")) for y in files])\n",
    "        files = [f\"{y}.txt\" for y in years if y >= re_start]\n",
    "    \n",
    "    # files = [\"2001.txt\"]\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for file in files:\n",
    "        print()\n",
    "        print(file)\n",
    "        with open(directory / file, encoding=\"utf-8\") as f, open(vector_dir / file, \"w\", encoding=\"utf-8\") as out:\n",
    "            for i, line in enumerate(f):\n",
    "                if i % 10 == 0:\n",
    "                    print(i, end=\"\\r\")\n",
    "                lemma, n, sentence = tuple(line.strip(\"\\n\").split(\"\\t\"))\n",
    "\n",
    "                sentence = map_tok(sentence)\n",
    "\n",
    "                if int(n) == 1: \n",
    "                    if lemma.startswith(ignore):\n",
    "                        continue\n",
    "                    if lemma in paradigms:\n",
    "                        regex = paradigms[lemma]\n",
    "                        regex = re.compile(regex)    \n",
    "                        exact_match = re.search(regex, sentence)\n",
    "                        if exact_match == None:\n",
    "                            print(\"ERROR:\", lemma, \"||\", sentence)\n",
    "                        exact_match = exact_match.group()                        \n",
    "                    else:\n",
    "                        regex = re.compile(f\"\\\\b[0-9a-zåäö]*{lemma.split('_')[-1]}.*?\\\\b\")\n",
    "                        exact_match = re.search(regex, sentence)\n",
    "                        if exact_match == None:\n",
    "                            print(\"ERROR:\", lemma, \"|\", regex, \"|\", sentence)\n",
    "                        exact_match = exact_match.group()\n",
    "                    vector = get_word_vector(sentence, exact_match, lemma, model, device, only_check)\n",
    "                    if only_check or vector == None:\n",
    "                        continue\n",
    "                        \n",
    "                    vector = \" \".join([str(v) for v in vector.tolist()]) # consider torch.save(tensor, 'file.pt')\n",
    "                    out.write(f\"{lemma}\\t{vector}\\n\")                   \n",
    "\n",
    "                else: # two instances of the same lemma = problem\n",
    "                    for l in lemma.split(\"; \"):\n",
    "                        if l.startswith(ignore):\n",
    "                            continue                        \n",
    "                        if l in paradigms:\n",
    "                            regex = paradigms[l]\n",
    "                            regex = re.compile(regex)    \n",
    "                            exact_match = re.search(regex, sentence).group() \n",
    "                        else:\n",
    "                            regex = re.compile(f\"\\\\b[0-9a-zåäö]*{l.split('_')[-1]}.*?\\\\b\")\n",
    "                            exact_match = re.search(regex, sentence).group()\n",
    "                        vector = get_word_vector(sentence, exact_match, l, model, device, only_check)\n",
    "                        if only_check or vector == None:\n",
    "                            continue\n",
    "                        vector = \" \".join([str(v) for v in vector.tolist()])\n",
    "                        out.write(f\"{l}\\t{vector}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flashback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"KBLab/megatron-bert-large-swedish-cased-110k\" #\"KBLab/megatron-bert-large-swedish-cased-165k\" #'KB/bert-base-swedish-cased'\n",
    "#model_name = 'KB/bert-base-swedish-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "short_name = model_name.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "get_word_embeddings(\n",
    "    model=model, \n",
    "    directory=Path(\"../data/corpus/fb_pol_files/\"), #\"C:\\Users\\xbohma\\Desktop\\work\\work\\data\\corpus\\files\"\n",
    "    vector_dir=f\"../data/vectors/fb_pol/{short_name}\", #\"../bert-base-swedish-cased/\", # \"C:\\Users\\xbohma\\Desktop\\work\\work\\data\\vectors\\bert-base-swedish-cased\"\n",
    "    paradigms=paradigms,\n",
    "    ignore=ignore,\n",
    "    device=\"cuda\",\n",
    "    only_check = False,\n",
    "    #re_start = 2018\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# get_word_embeddings(\n",
    "#     directory=Path(\"../files\"),\n",
    "#     vector_dir=\"../bert-base-swedish-cased/\",\n",
    "#     paradigms=paradigms,\n",
    "#     ignore=ignore,\n",
    "#     mode=\"bert\",\n",
    "#     device=\"cpu\",\n",
    "#     only_check = True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Familjeliv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"KBLab/megatron-bert-large-swedish-cased-110k\" #\"KBLab/megatron-bert-large-swedish-cased-165k\" #'KB/bert-base-swedish-cased'\n",
    "#model_name = 'KB/bert-base-swedish-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "short_name = model_name.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "get_word_embeddings(\n",
    "    model=model, \n",
    "    directory=Path(\"../data/corpus/fm_smh_files/\"), #\"C:\\Users\\xbohma\\Desktop\\work\\work\\data\\corpus\\files\"\n",
    "    vector_dir=f\"../data/vectors/ff_smh/{short_name}\", #\"../bert-base-swedish-cased/\", # \"C:\\Users\\xbohma\\Desktop\\work\\work\\data\\vectors\\bert-base-swedish-cased\"\n",
    "    paradigms=paradigms,\n",
    "    ignore=ignore,\n",
    "    device=\"cpu\",\n",
    "    only_check = True,\n",
    "    #re_start = 2018\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
